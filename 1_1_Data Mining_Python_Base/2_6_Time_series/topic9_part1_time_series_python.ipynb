{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"../../img/ods_stickers.jpg\" />\n",
    "    \n",
    "## [mlcourse.ai](https://mlcourse.ai) – Отворен курс за машинно обучение\n",
    "\n",
    "\n",
    "Автор: [Дмитрий Сергеев](https://github.com/DmitrySerg), Data Scientist @ Zeptolab, преподавател в Центъра по математически финанси в MSU. Преведено от: @borowis. Този материал е предмет на правилата и условията на лиценза [Creative Commons CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/). Безплатното използване е разрешено за всякакви нетърговски цели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>9. Анализ на времеви редове в Python</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здрасти!\n",
    "\n",
    "Продължаваме нашия отворен курс за машинно обучение с нова статия за времеви редове.\n",
    "\n",
    "Нека да разгледаме как да работим с времеви редове в Python: какви методи и модели можем да използваме за прогнозиране, какво е двойно и тройно експоненциално изглаждане, какво да правите, ако стационарността не е любимото ви нещо, как да изградите SARIMA и да останете живи , как да правите прогнози с помощта на xgboost... В допълнение, всичко това ще бъде приложено към (сурови) примери от реалния свят."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Описание на статията:\n",
    "1. [Въведение](#Въведение)\n",
    "   - [Показатели за качество на прогнозата](#Forecast-quality-metrics)\n",
    "2. [Преместване, изглаждане, оценка] (#Преместване,-изглаждане,-оценяване)\n",
    "   - Оценки на подвижния прозорец\n",
    "   - Експоненциално изглаждане, модел на Holt-Winters\n",
    "   - Кръстосано валидиране на времеви серии, избор на параметри\n",
    "3. [Иконометричен подход](#Econometric-approach)\n",
    "   - Стационарност, единичен корен\n",
    "   - Отърваване от нестационарността\n",
    "   - Интуиция и моделиране на SARIMA\n",
    "4. [Линейни (и не съвсем) модели за времеви серии](#Linear-(and-not-quite)-models-for-time-series)\n",
    "   - [Извличане на функции] (#Извличане на функции)\n",
    "   - [Закъснения във времеви серии](#Time-series-lags)\n",
    "   - [Целево кодиране](#Target-encoding)\n",
    "   - [Регулиране и избор на функции](#Регулиране-и-избор на функции)\n",
    "   - [Подсилване] (#Подсилване)\n",
    "5. [Заключение](#Заключение)\n",
    "6. [Демо задание](#Демо-задаване)\n",
    "7. [Полезни ресурси](#Полезни-ресурси)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В ежедневната си работа почти всеки ден се сблъсквам със задачи, свързани с времеви серии. Най-често задаваните въпроси са следните: какво ще се случи с нашите показатели през следващия ден/седмица/месец/и т.н., колко потребители ще инсталират нашето приложение, колко време ще прекарат онлайн, колко действия ще извършат потребителите, и така нататък. Можем да подходим към тези задачи за прогнозиране, като използваме различни методи в зависимост от необходимото качество на прогнозата, продължителността на прогнозния период и, разбира се, времето, в рамките на което трябва да изберем функции и да настроим параметрите, за да постигнем желаните резултати.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Въведение\n",
    "\n",
    "Започваме с проста [дефиниция](https://en.wikipedia.org/wiki/Time_series) на времеви редове:\n",
    "> *Времевата поредица* е поредица от точки от данни, индексирани (или изброени или изобразени на графики) във времеви ред.\n",
    "\n",
    "Следователно данните са организирани чрез относително детерминистични времеви клейма и могат, в сравнение с произволни примерни данни, да съдържат допълнителна информация, която можем да извлечем."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нека импортираме някои библиотеки. Първо, ще ни трябва библиотеката [statsmodels](http://statsmodels.sourceforge.net/stable/), която има много функции за статистическо моделиране, включително времеви редове. За любителите на R, които трябваше да преминат към Python, `statsmodels` определено ще изглежда по-познат, тъй като поддържа дефиниции на модели като \"Заплата ~ Възраст + Образование\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt  # plots\n",
    "import numpy as np  # vectors and matrices\n",
    "import pandas as pd  # tables and data manipulations\n",
    "import seaborn as sns  # more plots\n",
    "\n",
    "sns.set()\n",
    "\n",
    "import warnings  \n",
    "from itertools import product  # some useful functions\n",
    "\n",
    "import scipy.stats as scs\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf  # statistics and econometrics\n",
    "import statsmodels.tsa.api as smt\n",
    "from dateutil.relativedelta import \\\n",
    "    relativedelta  # working with dates with style\n",
    "from scipy.optimize import minimize  # for function minimization\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "warnings.filterwarnings(\"ignore\") # `do not disturbe` mode\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Като пример, нека да разгледаме реални данни за мобилни игри. По-конкретно, ще разгледаме гледаните реклами на час и разходите за валута в играта на ден:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ads = pd.read_csv(\"../../data/ads.csv\", index_col=[\"Time\"], parse_dates=[\"Time\"])\n",
    "currency = pd.read_csv(\n",
    "    \"../../data/currency.csv\", index_col=[\"Time\"], parse_dates=[\"Time\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(ads.Ads)\n",
    "plt.title(\"Ads watched (hourly data)\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 7))\n",
    "plt.plot(currency.GEMS_GEMS_SPENT)\n",
    "plt.title(\"In-game currency spent (daily data)\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Показатели за качество на прогнозата\n",
    "\n",
    "Преди да започнем да прогнозираме, нека разберем как да измерваме качеството на нашите прогнози и да разгледаме най-често използваните показатели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [R на квадрат](http://scikit-learn.org/stable/modules/model_evaluation.html#r2-score-the-coefficient-of-determination): коефициент на детерминация (в иконометрията това може да се тълкува като процент на дисперсията, обяснен от модела), $(-\\infty, 1]$\n",
    "\n",
    "$R^2 = 1 - \\frac{SS_{res}}{SS_{tot}}$\n",
    "\n",
    "``` python\n",
    "sklearn.metrics.r2_score\n",
    "```\n",
    "---\n",
    "- [Средна абсолютна грешка](http://scikit-learn.org/stable/modules/model_evaluation.html#mean-absolute-error): това е интерпретируема метрика, тъй като има същата мерна единица като първоначалната серия, $[0, +\\infty)$\n",
    "\n",
    "$MAE = \\frac{\\sum\\limits_{i=1}^{n} |y_i - \\hat{y}_i|}{n}$\n",
    "\n",
    "``` python\n",
    "sklearn.metrics.mean_absolute_error\n",
    "```\n",
    "---\n",
    "- [Средна абсолютна грешка](http://scikit-learn.org/stable/modules/model_evaluation.html#median-absolute-error): отново интерпретируем показател, който е особено интересен, защото е стабилен към извънредни стойности, $[ 0, +\\infty)$\n",
    "$MedAE = median(|y_1 - \\hat{y}_1|, ... , |y_n - \\hat{y}_n|)$\n",
    "\n",
    "```python\n",
    "sklearn.metrics.median_absolute_error\n",
    "```\n",
    "---\n",
    "- [Средна квадратна грешка](http://scikit-learn.org/stable/modules/model_evaluation.html#mean-squared-error): най-често използваният показател, който дава по-високо наказание за големи грешки и обратно, $ [0, +\\infty)$\n",
    "\n",
    "$MSE = \\frac{1}{n}\\sum\\limits_{i=1}^{n} (y_i - \\hat{y}_i)^2$\n",
    "\n",
    "``` python\n",
    "sklearn.metrics.mean_squared_error\n",
    "```\n",
    "---\n",
    "- [Средна квадратна логаритмична грешка](http://scikit-learn.org/stable/modules/model_evaluation.html#mean-squared-logarithmic-error): на практика това е същото като MSE, но вземаме логаритъма от сериала. В резултат на това придаваме по-голяма тежест и на малките грешки. Това обикновено се използва, когато данните имат експоненциални тенденции, $[0, +\\infty)$\n",
    "\n",
    "$MSLE = \\frac{1}{n}\\sum\\limits_{i=1}^{n} (log(1+y_i) - log(1+\\hat{y}_i))^2$\n",
    "\n",
    "``` python\n",
    "sklearn.metrics.mean_squared_log_error\n",
    "```\n",
    "---\n",
    "- Средна абсолютна процентна грешка: това е същото като MAE, но се изчислява като процент, което е много удобно, когато искате да обясните качеството на модела на ръководството, $[0, +\\infty)$\n",
    "\n",
    "$MAPE = \\frac{100}{n}\\sum\\limits_{i=1}^{n} \\frac{|y_i - \\hat{y}_i|}{y_i}$\n",
    "\n",
    "```python\n",
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing everything from above\n",
    "\n",
    "from sklearn.metrics import (mean_absolute_error, mean_squared_error,\n",
    "                             mean_squared_log_error, median_absolute_error,\n",
    "                             r2_score)\n",
    "\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "След като вече знаем как да измерваме качеството на прогнозите, нека да видим какви показатели можем да използваме и как да преведем резултатите за шефа. След това остава един малък детайл - изграждане на модела."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Move, smoothe, evaluate (Преместете, изгладете, оценете)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Да започнем с една наивна хипотеза: \"утре ще бъде същото като днес\". Въпреки това, вместо модел като $\\hat{y}_{t} = y_{t-1}$ (който всъщност е страхотна базова линия за всякакви проблеми с прогнозирането на времеви редове и понякога е невъзможно да се победи), ще приемем, че бъдещата стойност на нашата променлива зависи от средната стойност на нейните $k$ предишни стойности. Затова ще използваме **пълзяща средна**.\n",
    "\n",
    "$\\hat{y}_{t} = \\frac{1}{k} \\displaystyle\\sum^{k}_{n=1} y_{t-n}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def moving_average(series, n):\n",
    "    \"\"\"\n",
    "        Calculate average of last n observations\n",
    "    \"\"\"\n",
    "    return np.average(series[-n:])\n",
    "\n",
    "\n",
    "moving_average(ads, 24)  # prediction for the last observed day (past 24 hours)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "За съжаление не можем да правим прогнози далеч в бъдещето - за да получим стойността за следващата стъпка, трябва действително да се наблюдават предишните стойности. Но пълзящата средна има и друг случай на използване - изглаждане на оригиналния времеви ред за идентифициране на тенденции. Pandas има налична реализация с [`DataFrame.rolling(window).mean()`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.rolling.html). Колкото по-широк е прозорецът, толкова по-плавна е тенденцията. В случай на много шумни данни, които често се срещат във финансите, тази процедура може да помогне за откриване на общи модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def plotMovingAverage(\n",
    "    series, window, plot_intervals=False, scale=1.96, plot_anomalies=False\n",
    "):\n",
    "\n",
    "    \"\"\"\n",
    "        series - dataframe with timeseries\n",
    "        window - rolling window size \n",
    "        plot_intervals - show confidence intervals\n",
    "        plot_anomalies - show anomalies \n",
    "\n",
    "    \"\"\"\n",
    "    rolling_mean = series.rolling(window=window).mean()\n",
    "\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.title(\"Moving average\\n window size = {}\".format(window))\n",
    "    plt.plot(rolling_mean, \"g\", label=\"Rolling mean trend\")\n",
    "\n",
    "    # Plot confidence intervals for smoothed values\n",
    "    if plot_intervals:\n",
    "        mae = mean_absolute_error(series[window:], rolling_mean[window:])\n",
    "        deviation = np.std(series[window:] - rolling_mean[window:])\n",
    "        lower_bond = rolling_mean - (mae + scale * deviation)\n",
    "        upper_bond = rolling_mean + (mae + scale * deviation)\n",
    "        plt.plot(upper_bond, \"r--\", label=\"Upper Bond / Lower Bond\")\n",
    "        plt.plot(lower_bond, \"r--\")\n",
    "\n",
    "        # Having the intervals, find abnormal values\n",
    "        if plot_anomalies:\n",
    "            anomalies = pd.DataFrame(index=series.index, columns=series.columns)\n",
    "            anomalies[series < lower_bond] = series[series < lower_bond]\n",
    "            anomalies[series > upper_bond] = series[series > upper_bond]\n",
    "            plt.plot(anomalies, \"ro\", markersize=10)\n",
    "\n",
    "    plt.plot(series[window:], label=\"Actual values\")\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нека изгладим предходните 4 часа."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotMovingAverage(ads, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сега нека опитаме да изгладим предходните 12 часа.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotMovingAverage(ads, 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сега, изглаждайки предходните 24 часа, получаваме дневната тенденция.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotMovingAverage(ads, 24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Когато прилагахме ежедневно изглаждане на почасовите данни, можехме ясно да видим динамиката на гледаните реклами. През уикендите стойностите са по-високи (повече време за игра през уикендите), докато през делничните дни се гледат по-малко реклами."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можем също така да начертаем доверителни интервали за нашите изгладени стойности."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotMovingAverage(ads, 4, plot_intervals=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сега нека създадем проста система за откриване на аномалии с помощта на пълзяща средна. За съжаление, в този конкретен набор от данни всичко е повече или по-малко нормално, така че умишлено ще направим една от стойностите необичайна в нашата рамка с данни „ads_anomaly“."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ads_anomaly = ads.copy()\n",
    "ads_anomaly.iloc[-20] = ads_anomaly.iloc[-20] * 0.2  # say we have 80% drop of ads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нека да видим дали този прост метод може да хване аномалията."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotMovingAverage(ads_anomaly, 4, plot_intervals=True, plot_anomalies=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чудесно! Ами втората серия?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotMovingAverage(\n",
    "    currency, 7, plot_intervals=True, plot_anomalies=True\n",
    ")  # weekly smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "О, не, това не беше толкова страхотно! Тук можем да видим недостатъка на нашия прост подход – той не улови месечната сезонност в нашите данни и отбеляза почти всички 30-дневни пикове като аномалии. Ако искате да избегнете фалшиви положителни резултати, най-добре е да помислите за по-сложни модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Претеглена средна стойност** е проста модификация на подвижната средна. Теглата се сумират до „1“ с по-големи тегла, присвоени на по-скорошни наблюдения.\n",
    "\n",
    "\n",
    "$\\hat{y}_{t} = \\displaystyle\\sum^{k}_{n=1} \\omega_n y_{t+1-n}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def weighted_average(series, weights):\n",
    "    \"\"\"\n",
    "        Calculate weighted average on the series.\n",
    "        Assuming weights are sorted in descending order\n",
    "        (larger weights are assigned to more recent observations).\n",
    "    \"\"\"\n",
    "    result = 0.0\n",
    "    for n in range(len(weights)):\n",
    "        result += series.iloc[-n - 1] * weights[n]\n",
    "    return float(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_average(ads, [0.6, 0.3, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just checking\n",
    "0.6 * ads.iloc[-1] + 0.3 * ads.iloc[-2] + 0.1 * ads.iloc[-3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exponential smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сега, нека да видим какво ще се случи, ако вместо да претегляме последните $k$ стойности на времевия ред, започнем да претегляме всички налични наблюдения, докато експоненциално намаляваме теглата, докато се движим по-назад във времето. Съществува формула за **[експоненциално изглаждане](https://en.wikipedia.org/wiki/Exponential_smoothing)**, която ще ни помогне с това:\n",
    "\n",
    "$$\\hat{y}_{t} = \\alpha \\cdot y_t + (1-\\alpha) \\cdot \\hat y_{t-1} $$\n",
    "\n",
    "Тук стойността на модела е среднопретеглена стойност между текущата истинска стойност и стойностите на предишния модел. Теглото $\\alpha$ се нарича изглаждащ фактор. Той определя колко бързо ще \"забравим\" последното налично вярно наблюдение. Колкото по-малък е $\\alpha$, толкова по-голямо влияние имат предишните наблюдения и толкова по-плавна е серията.\n",
    "\n",
    "Експоненциалността се крие в рекурсивността на функцията – ние умножаваме по $(1-\\alpha)$ всеки път, което вече съдържа умножение по $(1-\\alpha)$ на предишни стойности на модела."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def exponential_smoothing(series, alpha):\n",
    "    \"\"\"\n",
    "        series - dataset with timestamps\n",
    "        alpha - float [0.0, 1.0], smoothing parameter\n",
    "    \"\"\"\n",
    "    result = [series[0]]  # first value is same as series\n",
    "    for n in range(1, len(series)):\n",
    "        result.append(alpha * series[n] + (1 - alpha) * result[n - 1])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def plotExponentialSmoothing(series, alphas):\n",
    "    \"\"\"\n",
    "        Plots exponential smoothing with different alphas\n",
    "        \n",
    "        series - dataset with timestamps\n",
    "        alphas - list of floats, smoothing parameters\n",
    "        \n",
    "    \"\"\"\n",
    "    with plt.style.context(\"seaborn-white\"):\n",
    "        plt.figure(figsize=(15, 7))\n",
    "        for alpha in alphas:\n",
    "            plt.plot(\n",
    "                exponential_smoothing(series, alpha), label=\"Alpha {}\".format(alpha)\n",
    "            )\n",
    "        plt.plot(series.values, \"c\", label=\"Actual\")\n",
    "        plt.legend(loc=\"best\")\n",
    "        plt.axis(\"tight\")\n",
    "        plt.title(\"Exponential Smoothing\")\n",
    "        plt.grid(True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotExponentialSmoothing(ads.Ads, [0.3, 0.05])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotExponentialSmoothing(currency.GEMS_GEMS_SPENT, [0.3, 0.05])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Double exponential smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Досега методите, които обсъждахме, бяха за прогнозиране на една бъдеща точка (с известно изглаждане). Това е готино, но също така не е достатъчно. Нека разширим експоненциалното изглаждане, така че да можем да предвидим две бъдещи точки (разбира се, ще включим и повече изглаждане).\n",
    "\n",
    "Разлагането на серията ще ни помогне -- получаваме два компонента: отсечка (т.е. ниво) $\\ell$ и наклон (т.е. тенденция) $b$. Научихме се да прогнозираме прихващане (или очаквана стойност на серията) с нашите предишни методи; сега ще приложим същото експоненциално изглаждане към тенденцията, като приемем, че бъдещата посока на промените във времевия ред зависи от предишните претеглени промени. В резултат на това получаваме следния набор от функции:\n",
    "\n",
    "$$\\ell_x = \\alpha y_x + (1-\\alpha)(\\ell_{x-1} + b_{x-1})$$\n",
    "\n",
    "$$b_x = \\beta(\\ell_x - \\ell_{x-1}) + (1-\\beta)b_{x-1}$$\n",
    "\n",
    "$$\\hat{y}_{x+1} = \\ell_x + b_x$$\n",
    "\n",
    "Първият описва пресечката, която, както и преди, зависи от текущата стойност на серията. Вторият член вече е разделен на предишни стойности на нивото и на тренда. Втората функция описва тенденцията, която зависи от промените на нивото на текущата стъпка и от предишната стойност на тенденцията. В този случай коефициентът $\\beta$ е тегло за експоненциално изглаждане. Крайната прогноза е сумата от стойностите на модела на пресечната точка и тренда."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     20
    ]
   },
   "outputs": [],
   "source": [
    "def double_exponential_smoothing(series, alpha, beta):\n",
    "    \"\"\"\n",
    "        series - dataset with timeseries\n",
    "        alpha - float [0.0, 1.0], smoothing parameter for level\n",
    "        beta - float [0.0, 1.0], smoothing parameter for trend\n",
    "    \"\"\"\n",
    "    # first value is same as series\n",
    "    result = [series[0]]\n",
    "    for n in range(1, len(series) + 1):\n",
    "        if n == 1:\n",
    "            level, trend = series[0], series[1] - series[0]\n",
    "        if n >= len(series):  # forecasting\n",
    "            value = result[-1]\n",
    "        else:\n",
    "            value = series[n]\n",
    "        last_level, level = level, alpha * value + (1 - alpha) * (level + trend)\n",
    "        trend = beta * (level - last_level) + (1 - beta) * trend\n",
    "        result.append(level + trend)\n",
    "    return result\n",
    "\n",
    "\n",
    "def plotDoubleExponentialSmoothing(series, alphas, betas):\n",
    "    \"\"\"\n",
    "        Plots double exponential smoothing with different alphas and betas\n",
    "        \n",
    "        series - dataset with timestamps\n",
    "        alphas - list of floats, smoothing parameters for level\n",
    "        betas - list of floats, smoothing parameters for trend\n",
    "    \"\"\"\n",
    "\n",
    "    with plt.style.context(\"seaborn-white\"):\n",
    "        plt.figure(figsize=(20, 8))\n",
    "        for alpha in alphas:\n",
    "            for beta in betas:\n",
    "                plt.plot(\n",
    "                    double_exponential_smoothing(series, alpha, beta),\n",
    "                    label=\"Alpha {}, beta {}\".format(alpha, beta),\n",
    "                )\n",
    "        plt.plot(series.values, label=\"Actual\")\n",
    "        plt.legend(loc=\"best\")\n",
    "        plt.axis(\"tight\")\n",
    "        plt.title(\"Double Exponential Smoothing\")\n",
    "        plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotDoubleExponentialSmoothing(ads.Ads, alphas=[0.9, 0.02], betas=[0.9, 0.02])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotDoubleExponentialSmoothing(\n",
    "    currency.GEMS_GEMS_SPENT, alphas=[0.9, 0.02], betas=[0.9, 0.02]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сега трябва да настроим два параметъра: $\\alpha$ и $\\beta$. Първият отговаря за изглаждането на серията около тренда, а вторият за изглаждането на самия тренд. Колкото по-големи са стойностите, толкова по-голяма тежест ще имат най-новите наблюдения и толкова по-малко изгладена ще бъде серията от модели. Някои комбинации от параметрите могат да доведат до странни резултати, особено ако са зададени ръчно. След малко ще разгледаме автоматичното избиране на параметри; преди това нека обсъдим тройното експоненциално изглаждане.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Тройно експоненциално изглаждане, известно още като Holt-Winters\n",
    "\n",
    "Разгледахме експоненциалното изглаждане и двойното експоненциално изглаждане. Този път преминаваме към _тройно_ експоненциално изглаждане.\n",
    "\n",
    "Както се досещате, идеята е да се добави и трети компонент – сезонността. Това означава, че не трябва да използваме този метод, ако не се очаква нашите времеви редове да имат сезонност. Сезонните компоненти в модела ще обяснят повтарящите се вариации около отсечката и тенденцията и ще бъдат определени от продължителността на сезона, с други думи от периода, след който вариациите се повтарят. За всяко наблюдение в сезона има отделен компонент; например, ако продължителността на сезона е 7 дни (седмична сезонност), ще имаме 7 сезонни компонента, по един за всеки ден от седмицата.\n",
    "\n",
    "С това нека напишем нова система от уравнения:\n",
    "\n",
    "$$\\ell_x = \\alpha(y_x - s_{x-L}) + (1-\\alpha)(\\ell_{x-1} + b_{x-1})$$\n",
    "\n",
    "$$b_x = \\beta(\\ell_x - \\ell_{x-1}) + (1-\\beta)b_{x-1}$$\n",
    "\n",
    "$$s_x = \\gamma(y_x - \\ell_x) + (1-\\gamma)s_{x-L}$$\n",
    "\n",
    "$$\\hat{y}_{x+m} = \\ell_x + mb_x + s_{x-L+1+(m-1)modL}$$\n",
    "\n",
    "Отсечката сега зависи от текущата стойност на серията минус всеки съответен сезонен компонент. Трендът остава непроменен, а сезонният компонент зависи от текущата стойност на серията минус пресечната точка и от предишната стойност на компонента. Имайте предвид, че компонентът се изглажда през всички налични сезони; например, ако имаме компонент понеделник, тогава той ще бъде осреднен само с други понеделници. Можете да прочетете повече за това как работи осредняването и как се извършва първоначалното приближение на тенденцията и сезонните компоненти [тук](http://www.itl.nist.gov/div898/handbook/pmc/section4/pmc435.htm). Сега, след като имаме сезонния компонент, можем да предвидим не само една или две стъпки напред, а произволни $m$ бъдещи стъпки напред, което е много обнадеждаващо.\n",
    "\n",
    "По-долу е кодът за модел на тройно експоненциално изглаждане, който е известен и с фамилните имена на своите създатели Чарлз Холт и неговия ученик Питър Уинтърс. Освен това, методът Brutlag беше включен в модела за получаване на доверителни интервали:\n",
    "\n",
    "$$\\hat y_{max_x}=\\ell_{x−1}+b_{x−1}+s_{x−T}+m⋅d_{t−T}$$\n",
    "\n",
    "$$\\hat y_{min_x}=\\ell_{x−1}+b_{x−1}+s_{x−T}-m⋅d_{t−T}$$\n",
    "\n",
    "$$d_t=\\gamma∣y_t−\\hat y_t∣+(1−\\gamma)d_{t−T},$$\n",
    "\n",
    "където $T$ е продължителността на сезона, $d$ е предвиденото отклонение. Други параметри са взети от тройно експоненциално изглаждане. Можете да прочетете повече за метода и неговата приложимост за откриване на аномалии във времеви серии [тук](http://fedcsis.org/proceedings/2012/pliks/118.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class HoltWinters:\n",
    "\n",
    "    \"\"\"\n",
    "    Модел на Holt-Winters с откриване на аномалии по метода Brutlag\n",
    "    \n",
    "    # серия - начална времева серия\n",
    "    # slen - продължителност на сезона\n",
    "    # алфа, бета, гама - коефициенти на модела на Холт-Уинтърс\n",
    "    # n_preds - хоризонт на прогнози\n",
    "    # scaling_factor - задава ширината на доверителния интервал чрез Brutlag (обикновено приема стойности от 2 до 3)\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, series, slen, alpha, beta, gamma, n_preds, scaling_factor=1.96):\n",
    "        self.series = series\n",
    "        self.slen = slen\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.n_preds = n_preds\n",
    "        self.scaling_factor = scaling_factor\n",
    "\n",
    "    def initial_trend(self):\n",
    "        sum = 0.0\n",
    "        for i in range(self.slen):\n",
    "            sum += float(self.series[i + self.slen] - self.series[i]) / self.slen\n",
    "        return sum / self.slen\n",
    "\n",
    "    def initial_seasonal_components(self):\n",
    "        seasonals = {}\n",
    "        season_averages = []\n",
    "        n_seasons = int(len(self.series) / self.slen)\n",
    "        # let's calculate season averages\n",
    "        for j in range(n_seasons):\n",
    "            season_averages.append(\n",
    "                sum(self.series[self.slen * j : self.slen * j + self.slen])\n",
    "                / float(self.slen)\n",
    "            )\n",
    "        # let's calculate initial values\n",
    "        for i in range(self.slen):\n",
    "            sum_of_vals_over_avg = 0.0\n",
    "            for j in range(n_seasons):\n",
    "                sum_of_vals_over_avg += (\n",
    "                    self.series[self.slen * j + i] - season_averages[j]\n",
    "                )\n",
    "            seasonals[i] = sum_of_vals_over_avg / n_seasons\n",
    "        return seasonals\n",
    "\n",
    "    def triple_exponential_smoothing(self):\n",
    "        self.result = []\n",
    "        self.Smooth = []\n",
    "        self.Season = []\n",
    "        self.Trend = []\n",
    "        self.PredictedDeviation = []\n",
    "        self.UpperBond = []\n",
    "        self.LowerBond = []\n",
    "\n",
    "        seasonals = self.initial_seasonal_components()\n",
    "\n",
    "        for i in range(len(self.series) + self.n_preds):\n",
    "            if i == 0:  # components initialization\n",
    "                smooth = self.series[0]\n",
    "                trend = self.initial_trend()\n",
    "                self.result.append(self.series[0])\n",
    "                self.Smooth.append(smooth)\n",
    "                self.Trend.append(trend)\n",
    "                self.Season.append(seasonals[i % self.slen])\n",
    "\n",
    "                self.PredictedDeviation.append(0)\n",
    "\n",
    "                self.UpperBond.append(\n",
    "                    self.result[0] + self.scaling_factor * self.PredictedDeviation[0]\n",
    "                )\n",
    "\n",
    "                self.LowerBond.append(\n",
    "                    self.result[0] - self.scaling_factor * self.PredictedDeviation[0]\n",
    "                )\n",
    "                continue\n",
    "\n",
    "            if i >= len(self.series):  # predicting\n",
    "                m = i - len(self.series) + 1\n",
    "                self.result.append((smooth + m * trend) + seasonals[i % self.slen])\n",
    "\n",
    "                # when predicting we increase uncertainty on each step\n",
    "                self.PredictedDeviation.append(self.PredictedDeviation[-1] * 1.01)\n",
    "\n",
    "            else:\n",
    "                val = self.series[i]\n",
    "                last_smooth, smooth = (\n",
    "                    smooth,\n",
    "                    self.alpha * (val - seasonals[i % self.slen])\n",
    "                    + (1 - self.alpha) * (smooth + trend),\n",
    "                )\n",
    "                trend = self.beta * (smooth - last_smooth) + (1 - self.beta) * trend\n",
    "                seasonals[i % self.slen] = (\n",
    "                    self.gamma * (val - smooth)\n",
    "                    + (1 - self.gamma) * seasonals[i % self.slen]\n",
    "                )\n",
    "                self.result.append(smooth + trend + seasonals[i % self.slen])\n",
    "\n",
    "                # Deviation is calculated according to Brutlag algorithm.\n",
    "                self.PredictedDeviation.append(\n",
    "                    self.gamma * np.abs(self.series[i] - self.result[i])\n",
    "                    + (1 - self.gamma) * self.PredictedDeviation[-1]\n",
    "                )\n",
    "\n",
    "            self.UpperBond.append(\n",
    "                self.result[-1] + self.scaling_factor * self.PredictedDeviation[-1]\n",
    "            )\n",
    "\n",
    "            self.LowerBond.append(\n",
    "                self.result[-1] - self.scaling_factor * self.PredictedDeviation[-1]\n",
    "            )\n",
    "\n",
    "            self.Smooth.append(smooth)\n",
    "            self.Trend.append(trend)\n",
    "            self.Season.append(seasonals[i % self.slen])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Кръстосано валидиране на времеви редове\n",
    "\n",
    "Преди да започнем да изграждаме модел, нека първо обсъдим как автоматично да оценяваме параметрите на модела.\n",
    "\n",
    "Тук няма нищо необичайно; както винаги, трябва да изберем функция на загубите, подходяща за задачата, която ще ни каже колко точно моделът приближава данните. След това, използвайки кръстосано валидиране, ще оценим избраната от нас функция на загуба за дадените параметри на модела, ще изчислим градиента, ще коригираме параметрите на модела и т.н., като в крайна сметка ще се спуснем до глобалния минимум.\n",
    "\n",
    "Може да питате как да направите кръстосано валидиране за времеви редове, тъй като времевите редове имат тази времева структура и човек не може произволно да смесва стойности в гънка, като запазва тази структура. С рандомизирането всички времеви зависимости между наблюденията ще бъдат загубени. Ето защо ще трябва да използваме по-сложен подход при оптимизиране на параметрите на модела. Не знам дали има официално име за това, но на [CrossValidated](https://stats.stackexchange.com/questions/14099/using-k-fold-cross-validation-for-time-series-model -селекция), където можете да намерите всички отговори, освен отговора на основния въпрос за живота, вселената и всичко, предложеното име за този метод е „кръстосано валидиране на непрекъсната основа“.\n",
    "\n",
    "Идеята е доста проста -- обучаваме нашия модел на малък сегмент от времевия ред от началото до някои $t$, правим прогнози за следващите $t+n$ стъпки и изчисляваме грешка. След това разширяваме нашата обучителна извадка до $t+n$ стойност, правим прогнози от $t+n$ до $t+2*n$ и продължаваме да преместваме нашия тестов сегмент от времевия ред, докато достигнем последното налично наблюдение. В резултат на това имаме толкова гънки, колкото $n$ ще се поберат между първоначалната тренировъчна извадка и последното наблюдение.\n",
    "\n",
    "<img src=\"../../img/time_series_cv.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сега, знаейки как да настроим кръстосано валидиране, можем да намерим оптималните параметри за модела на Holt-Winters. Спомнете си, че имаме ежедневна сезонност в рекламите, оттук и параметърът `slen=24`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     2
    ]
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mean_squared_error' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \\\n\u001b[1;32m      2\u001b[0m     TimeSeriesSplit  \u001b[38;5;66;03m# you have everything done for you\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtimeseriesCVscore\u001b[39m(params, series, loss_function\u001b[38;5;241m=\u001b[39mmean_squared_error, slen\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m24\u001b[39m):\n\u001b[1;32m      6\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;03m        Връща грешка в CV\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;03m        \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03m        slen - сезонна дължина за модел Holt-Winters\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# errors array\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mean_squared_error' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import \\\n",
    "    TimeSeriesSplit  # you have everything done for you\n",
    "\n",
    "\n",
    "def timeseriesCVscore(params, series, loss_function=mean_squared_error, slen=24):\n",
    "    \"\"\"\n",
    "        Връща грешка в CV\n",
    "        \n",
    "        params - вектор от параметри за оптимизация\n",
    "        серия - набор от данни с времеви серии\n",
    "        slen - сезонна дължина за модел Holt-Winters\n",
    "    \"\"\"\n",
    "    # errors array\n",
    "    errors = []\n",
    "\n",
    "    values = series.values\n",
    "    alpha, beta, gamma = params\n",
    "\n",
    "    # set the number of folds for cross-validation\n",
    "    tscv = TimeSeriesSplit(n_splits=3)\n",
    "\n",
    "    # iterating over folds, train model on each, forecast and calculate error\n",
    "    for train, test in tscv.split(values):\n",
    "\n",
    "        model = HoltWinters(\n",
    "            series=values[train],\n",
    "            slen=slen,\n",
    "            alpha=alpha,\n",
    "            beta=beta,\n",
    "            gamma=gamma,\n",
    "            n_preds=len(test),\n",
    "        )\n",
    "        model.triple_exponential_smoothing()\n",
    "\n",
    "        predictions = model.result[-len(test) :]\n",
    "        actual = values[test]\n",
    "        error = loss_function(predictions, actual)\n",
    "        errors.append(error)\n",
    "\n",
    "    return np.mean(np.array(errors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В модела на Холт-Уинтърс, както и в другите модели на експоненциално изглаждане, има ограничение за това колко големи могат да бъдат параметрите на изглаждане, като всеки от тях варира от 0 до 1. Следователно, за да минимизираме нашата функция на загубите, ние трябва да изберете алгоритъм, който поддържа ограничения върху параметрите на модела. В нашия случай ще използваме пресечения спрегнат градиент на Нютон.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data = ads.Ads[:-20]  # leave some data for testing\n",
    "\n",
    "# инициализиране на параметрите на модела алфа, бета и гама\n",
    "x = [0, 0, 0]\n",
    "\n",
    "# Минимизиране на функцията за загуба\n",
    "opt = minimize(\n",
    "    timeseriesCVscore,\n",
    "    x0=x,\n",
    "    args=(data, mean_squared_log_error),\n",
    "    method=\"TNC\",\n",
    "    bounds=((0, 1), (0, 1), (0, 1)),\n",
    ")\n",
    "\n",
    "# Вземете оптимални стойности...\n",
    "alpha_final, beta_final, gamma_final = opt.x\n",
    "print(alpha_final, beta_final, gamma_final)\n",
    "\n",
    "# ...и пуснете train на тях, като прогнозирате за следващите 50 часа\n",
    "model = HoltWinters(\n",
    "    data,\n",
    "    slen=24,\n",
    "    alpha=alpha_final,\n",
    "    beta=beta_final,\n",
    "    gamma=gamma_final,\n",
    "    n_preds=50,\n",
    "    scaling_factor=3,\n",
    ")\n",
    "model.triple_exponential_smoothing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нека добавим малко код за изобразяване на графики."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def plotHoltWinters(series, plot_intervals=False, plot_anomalies=False):\n",
    "    \"\"\"\n",
    "        series - dataset with timeseries\n",
    "        plot_intervals - show confidence intervals\n",
    "        plot_anomalies - show anomalies \n",
    "    \"\"\"\n",
    "\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.plot(model.result, label=\"Model\")\n",
    "    plt.plot(series.values, label=\"Actual\")\n",
    "    error = mean_absolute_percentage_error(series.values, model.result[: len(series)])\n",
    "    plt.title(\"Mean Absolute Percentage Error: {0:.2f}%\".format(error))\n",
    "\n",
    "    if plot_anomalies:\n",
    "        anomalies = np.array([np.NaN] * len(series))\n",
    "        anomalies[series.values < model.LowerBond[: len(series)]] = series.values[\n",
    "            series.values < model.LowerBond[: len(series)]\n",
    "        ]\n",
    "        anomalies[series.values > model.UpperBond[: len(series)]] = series.values[\n",
    "            series.values > model.UpperBond[: len(series)]\n",
    "        ]\n",
    "        plt.plot(anomalies, \"o\", markersize=10, label=\"Anomalies\")\n",
    "\n",
    "    if plot_intervals:\n",
    "        plt.plot(model.UpperBond, \"r--\", alpha=0.5, label=\"Up/Low confidence\")\n",
    "        plt.plot(model.LowerBond, \"r--\", alpha=0.5)\n",
    "        plt.fill_between(\n",
    "            x=range(0, len(model.result)),\n",
    "            y1=model.UpperBond,\n",
    "            y2=model.LowerBond,\n",
    "            alpha=0.2,\n",
    "            color=\"grey\",\n",
    "        )\n",
    "\n",
    "    plt.vlines(\n",
    "        len(series),\n",
    "        ymin=min(model.LowerBond),\n",
    "        ymax=max(model.UpperBond),\n",
    "        linestyles=\"dashed\",\n",
    "    )\n",
    "    plt.axvspan(len(series) - 20, len(model.result), alpha=0.3, color=\"lightgrey\")\n",
    "    plt.grid(True)\n",
    "    plt.axis(\"tight\")\n",
    "    plt.legend(loc=\"best\", fontsize=13);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotHoltWinters(ads.Ads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotHoltWinters(ads.Ads, plot_intervals=True, plot_anomalies=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Съдейки по графиките, нашият модел успя успешно да приближи първоначалния времеви ред, като улови дневната сезонност, общата низходяща тенденция и дори някои аномалии. Ако погледнете отклоненията на модела, можете ясно да видите, че моделът реагира доста рязко на промените в структурата на серията, но след това бързо връща отклонението към нормалните стойности, като по същество \"забравя\" миналото. Тази характеристика на модела ни позволява бързо да изграждаме системи за откриване на аномалии, дори за шумни серии от данни, без да харчим твърде много време и пари за подготовка на данните и обучение на модела."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25, 5))\n",
    "plt.plot(model.PredictedDeviation)\n",
    "plt.grid(True)\n",
    "plt.axis(\"tight\")\n",
    "plt.title(\"Brutlag's predicted deviation\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ще приложим същия алгоритъм за втората серия, която, както може би си спомняте, има тенденция и 30-дневна сезонност."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data = currency.GEMS_GEMS_SPENT[:-50]\n",
    "slen = 30  # 30-day seasonality\n",
    "\n",
    "x = [0, 0, 0]\n",
    "\n",
    "opt = minimize(\n",
    "    timeseriesCVscore,\n",
    "    x0=x,\n",
    "    args=(data, mean_absolute_percentage_error, slen),\n",
    "    method=\"TNC\",\n",
    "    bounds=((0, 1), (0, 1), (0, 1)),\n",
    ")\n",
    "\n",
    "alpha_final, beta_final, gamma_final = opt.x\n",
    "print(alpha_final, beta_final, gamma_final)\n",
    "\n",
    "model = HoltWinters(\n",
    "    data,\n",
    "    slen=slen,\n",
    "    alpha=alpha_final,\n",
    "    beta=beta_final,\n",
    "    gamma=gamma_final,\n",
    "    n_preds=100,\n",
    "    scaling_factor=3,\n",
    ")\n",
    "model.triple_exponential_smoothing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotHoltWinters(currency.GEMS_GEMS_SPENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Изглежда добре! Моделът улови както възходяща тенденция, така и сезонни пикове и пасва на данните доста добре."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotHoltWinters(currency.GEMS_GEMS_SPENT, plot_intervals=True, plot_anomalies=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 5))\n",
    "plt.plot(model.PredictedDeviation)\n",
    "plt.grid(True)\n",
    "plt.axis(\"tight\")\n",
    "plt.title(\"Brutlag's predicted deviation\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Иконометричен подход\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Стационарност\n",
    "\n",
    "Преди да започнем моделирането, трябва да споменем едно толкова важно свойство на времевите редове: [**стационарност**](https://en.wikipedia.org/wiki/Stationary_process).\n",
    "\n",
    "Ако даден процес е стационарен, това означава, че той не променя своите статистически свойства с течение на времето, а именно средната стойност и дисперсията. (Постоянството на дисперсията се нарича [хомоскедастичност](https://en.wikipedia.org/wiki/Homoscedasticity)) Ковариационната функция не зависи от времето; трябва да зависи само от разстоянието между наблюденията. Можете да видите това визуално на изображенията в публикацията от [Sean Abu](http://www.seanabu.com/2016/03/22/time-series-seasonal-ARIMA-model-in-python/):\n",
    "\n",
    "- Червената графика по-долу не е неподвижна, защото средната стойност нараства с времето.\n",
    "\n",
    "<img src=\"https://habrastorage.org/files/20c/9d8/a63/20c9d8a633ec436f91dccd4aedcc6940.png\"/>\n",
    "\n",
    "- Нямахме късмет с дисперсията и виждаме различното разпространение на стойностите във времето\n",
    "\n",
    "<img src=\"https://habrastorage.org/files/b88/eec/a67/b88eeca676d642449cab135273fd5a95.png\"/>\n",
    "\n",
    "- И накрая, ковариацията на i-тия член и (i + m)-ия член не трябва да бъде функция на времето. В следващата графика ще забележите, че спредът се сближава с времето. Следователно ковариацията не е постоянна с времето в дясната диаграма.\n",
    "\n",
    "<img src=\"https://habrastorage.org/files/2f6/1ee/cb2/2f61eecb20714352840748b826e38680.png\"/>\n",
    "\n",
    "И така, защо стационарността е толкова важна? Тъй като е лесно да се правят прогнози за стационарни серии, тъй като можем да приемем, че бъдещите статистически свойства няма да се различават от наблюдаваните в момента. Повечето модели на времеви редове по един или друг начин се опитват да предскажат тези свойства (средна стойност или дисперсия, например). Бъдещите прогнози биха били погрешни, ако оригиналната серия не беше неподвижна. За съжаление повечето времеви редове, които виждаме извън учебниците, са нестационарни, но можем (и трябва) да променим това.\n",
    "\n",
    "Така че, за да се борим с нестационарността, трябва да познаваме врага си, така да се каже. Да видим как можем да го открием. Ще разгледаме белия шум и произволните разходки, за да научим как да стигнем от един до друг безплатно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Диаграма на белия шум:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "white_noise = np.random.normal(size=1000)\n",
    "with plt.style.context(\"bmh\"):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.plot(white_noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Процесът, генериран от стандартното нормално разпределение, е стационарен и осцилира около 0 с отклонение от 1. Сега, въз основа на този процес, ще генерираме нов, където всяка следваща стойност ще зависи от предишната: $x_t = \\rho x_ {t-1} + e_t$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ето кода за изобразяване на графиките."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def plotProcess(n_samples=1000, rho=0):\n",
    "    x = w = np.random.normal(size=n_samples)\n",
    "    for t in range(n_samples):\n",
    "        x[t] = rho * x[t - 1] + w[t]\n",
    "\n",
    "    with plt.style.context(\"bmh\"):\n",
    "        plt.figure(figsize=(10, 3))\n",
    "        plt.plot(x)\n",
    "        plt.title(\n",
    "            \"Rho {}\\n Dickey-Fuller p-value: {}\".format(\n",
    "                rho, round(sm.tsa.stattools.adfuller(x)[1], 3)\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "for rho in [0, 0.6, 0.9, 1]:\n",
    "    plotProcess(rho=rho)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На първия график можете да видите същия неподвижен бял шум, както преди. На втория график с $\\rho$, увеличен до 0,6, се появиха по-широки цикли, но все още изглежда стационарен като цяло. Третият график се отклонява още повече от средната стойност 0, но все още осцилира около средната стойност. И накрая, с $\\rho=1$, имаме процес на произволно ходене, т.е. нестационарен времеви ред.\n",
    "\n",
    "Това се случва, защото след достигане на критичната стойност серията $x_t = \\rho x_{t-1} + e_t$ не се връща към средната си стойност. Ако извадим $x_{t-1}$ от двете страни, ще получим $x_t - x_{t-1} = (\\rho - 1) x_{t-1} + e_t$, където изразът отляво се нарича първа разлика. Ако $\\rho=1$, тогава първата разлика ни дава стационарен бял шум $e_t$. Това е основната идея зад [теста на Дики-Фулър](https://en.wikipedia.org/wiki/Dickey%E2%80%93Fuller_test) за стационарност на времеви редове (тестване на наличието на единичен корен). Ако можем да получим стационарна серия от нестационарна серия, използвайки първата разлика, ние наричаме тези серии интегрирани от ред 1. Нулевата хипотеза на теста е, че времевият ред е нестационарен, което беше отхвърлено при първите три парцели и накрая приет на последния. Трябва да кажем, че първата разлика не винаги е достатъчна, за да получим стационарна серия, тъй като процесът може да бъде интегриран от порядък d, d > 1 (и да има множество единични корени). В такива случаи се използва разширеният тест на Дики-Фулър, който проверява няколко лага наведнъж.\n",
    "\n",
    "Можем да се борим с нестационарността, като използваме различни подходи: различни разлики в реда, премахване на тенденция и сезонност, изглаждане и трансформации като Box-Cox или логаритмични.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Отърваване от нестационарността и изграждане на SARIMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": []
   },
   "source": [
    "Нека изградим модел на ARIMA, като преминем през всички ~~кръгове на ада~~ етапи на създаване на серия неподвижна."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ето кода за изобразяване на графики."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def tsplot(y, lags=None, figsize=(12, 7), style=\"bmh\"):\n",
    "    \"\"\"\n",
    "        Начертайте времеви редове, техните ACF и PACF, изчислете теста на Дики-Фулър\n",
    "        \n",
    "        y - времеви серии\n",
    "        закъснения(lagg) - колко закъснения да се включат в изчислението на ACF, PACF\n",
    "    \"\"\"\n",
    "    if not isinstance(y, pd.Series):\n",
    "        y = pd.Series(y)\n",
    "\n",
    "    with plt.style.context(style):\n",
    "        fig = plt.figure(figsize=figsize)\n",
    "        layout = (2, 2)\n",
    "        ts_ax = plt.subplot2grid(layout, (0, 0), colspan=2)\n",
    "        acf_ax = plt.subplot2grid(layout, (1, 0))\n",
    "        pacf_ax = plt.subplot2grid(layout, (1, 1))\n",
    "\n",
    "        ts_ax.plot(y)\n",
    "        p_value = sm.tsa.stattools.adfuller(y)[1]\n",
    "        ts_ax.set_title(\n",
    "            \"Time Series Analysis Plots\\n Dickey-Fuller: p={0:.5f}\".format(p_value)\n",
    "        )\n",
    "        smt.graphics.plot_acf(y, lags=lags, ax=acf_ax)\n",
    "        smt.graphics.plot_pacf(y, lags=lags, ax=pacf_ax)\n",
    "        plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsplot(ads.Ads, lags=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_този отклонение на диаграмата на частична автокорелация изглежда като грешка в statsmodels, частичната автокорелация трябва да бъде <= 1 като всяка корелация._\n",
    "\n",
    "Изненадващо, първоначалните серии са неподвижни; тестът на Дики-Фулър отхвърли нулевата хипотеза, че е налице единичен корен. Всъщност можем да видим това на самата графика – нямаме видима тенденция, така че средната стойност е постоянна, а дисперсията е доста стабилна. Единственото, което остава, е сезонността, с която трябва да се справим преди моделирането. За да направим това, нека вземем \"сезонната разлика\", което означава просто изваждане на серията от самата нея с лаг, който е равен на сезонния период."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ads_diff = ads.Ads - ads.Ads.shift(24)\n",
    "tsplot(ads_diff[24:], lags=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сега е много по-добре с изчезналата видима сезонност. Функцията за автокорелация обаче все още има твърде много значителни закъснения. За да ги премахнем, ще вземем първите разлики, като извадим серията от самата нея със закъснение 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ads_diff = ads_diff - ads_diff.shift(1)\n",
    "tsplot(ads_diff[24 + 1 :], lags=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перфектно! Нашата серия сега изглежда като нещо неописуемо, осцилиращо около нулата. Тестът на Дики-Фулър показва, че е стационарен и броят на значимите пикове в ACF е намалял. Най-накрая можем да започнем да моделираме!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ARIMA-семеен интензивен курс\n",
    "\n",
    "Ще обясним този модел, като изграждаме буква по буква. $SARIMA(p, d, q)(P, D, Q, s)$, модел на сезонна авторегресия на подвижната средна:\n",
    "\n",
    "- $AR(p)$ - авторегресионен модел, т.е. регресия на времевия ред върху себе си. Основното предположение е, че текущите стойности на серията зависят от предишните стойности с известно закъснение (или няколко закъснения). Максималното забавяне в модела се означава като $p$. За да определите първоначалния $p$, трябва да погледнете графиката на PACF и да намерите най-голямото значимо закъснение, след което **повечето** други закъснения стават незначителни.\n",
    "- $MA(q)$ - модел на пълзяща средна. Без да навлизаме в много подробности, това моделира грешката на времевия ред, отново с предположението, че текущата грешка зависи от предишната с известно закъснение, което се нарича $q$. Първоначалната стойност може да бъде намерена на диаграмата на ACF със същата логика, както преди. \n",
    "\n",
    "Нека комбинираме нашите първи 4 букви:\n",
    "\n",
    "$AR(p) + MA(q) = ARMA(p, q)$\n",
    "\n",
    "Това, което имаме тук, е моделът на авторегресия – подвижна средна! Ако редицата е неподвижна, тя може да бъде апроксимирана с тези 4 букви. Да продължим.\n",
    "\n",
    "- $I(d)$ - ред на интегриране. Това е просто броят на несезонните разлики, необходими, за да бъде серията стационарна. В нашия случай това е само 1, защото използвахме първите разлики.\n",
    "\n",
    "Добавянето на тази буква към четирите ни дава модела $ARIMA$, който може да обработва нестационарни данни с помощта на несезонни разлики. Страхотно, остава още едно писмо!\n",
    "\n",
    "- $S(s)$ - това отговаря за сезонността и се равнява на продължителността на сезонния период на серията\n",
    "\n",
    "С това имаме три параметъра: $(P, D, Q)$\n",
    "\n",
    "- $P$ - ред на авторегресия за сезонния компонент на модела, който може да бъде извлечен от PACF. Но трябва да погледнете броя на значителните закъснения, които са кратни на продължителността на сезонния период. Например, ако периодът е равен на 24 и виждаме, че 24-ият и 48-ият лаг са значими в PACF, това означава, че първоначалният $P$ трябва да бъде 2.\n",
    "\n",
    "- $Q$ - подобна логика, използваща вместо това графиката на ACF.\n",
    "\n",
    "- $D$ - ред на сезонно интегриране. Това може да бъде равно на 1 или 0, в зависимост от това дали са приложени сезонни разлики или не."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "След като вече знаем как да зададем първоначалните параметри, нека отново да погледнем окончателния график и да зададем параметрите:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsplot(ads_diff[24 + 1 :], lags=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $p$ най-вероятно е 4, тъй като това е последното значително забавяне на PACF, след което повечето други не са значими.\n",
    "- $d$ е равно на 1, защото имахме първи разлики\n",
    "- $q$ трябва да е някъде около 4, както и да се вижда на ACF\n",
    "- $P$ може да е 2, тъй като 24-ти и 48-ми лагове са донякъде значими за PACF\n",
    "- $D$ отново е равно на 1, защото направихме сезонна диференциация\n",
    "- $Q$ вероятно е 1. 24-то забавяне на ACF е значително, докато 48-ото не е."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нека тестваме различни модели и да видим кой е по-добър."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# задаване на начални стойности и някои граници за тях\n",
    "ps = range(2, 5)\n",
    "d = 1\n",
    "qs = range(2, 5)\n",
    "Ps = range(0, 2)\n",
    "D = 1\n",
    "Qs = range(0, 2)\n",
    "s = 24 # Продължителността на сезона все още е 24\n",
    "# създаване на списък с всички възможни комбинации от параметри\n",
    "parameters = product(ps, qs, Ps, Qs)\n",
    "parameters_list = list(parameters)\n",
    "len(parameters_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimizeSARIMA(parameters_list, d, D, s):\n",
    "    \"\"\"\n",
    "        Върнете рамка с данни с параметри и съответния AIC\n",
    "        \n",
    "        parameters_list - списък с (p, q, P, Q) кортежи\n",
    "        d - ред на интегриране в ARIMA модел\n",
    "        D - ред за сезонно интегриране\n",
    "        s - продължителност на сезона\n",
    "    \"\"\"\n",
    "\n",
    "    results = []\n",
    "    best_aic = float(\"inf\")\n",
    "\n",
    "    for param in tqdm(parameters_list):\n",
    "    # трябва да опитаме - освен защото при някои комбинации моделът не успява да се сближи\n",
    "        try:\n",
    "            model = sm.tsa.statespace.SARIMAX(\n",
    "                ads.Ads,\n",
    "                order=(param[0], d, param[1]),\n",
    "                seasonal_order=(param[2], D, param[3], s),\n",
    "            ).fit(disp=-1)\n",
    "        except:\n",
    "            continue\n",
    "        aic = model.aic\n",
    "        # saving best model, AIC and parameters\n",
    "        if aic < best_aic:\n",
    "            best_model = model\n",
    "            best_aic = aic\n",
    "            best_param = param\n",
    "        results.append([param, model.aic])\n",
    "\n",
    "    result_table = pd.DataFrame(results)\n",
    "    result_table.columns = [\"parameters\", \"aic\"]\n",
    "# сортиране във възходящ ред, колкото по-нисък е AIC - толкова по-добре\n",
    "    result_table = result_table.sort_values(by=\"aic\", ascending=True).reset_index(\n",
    "        drop=True\n",
    "    )\n",
    "\n",
    "    return result_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "result_table = optimizeSARIMA(parameters_list, d, D, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the parameters that give the lowest AIC\n",
    "p, q, P, Q = result_table.parameters[0]\n",
    "\n",
    "best_model = sm.tsa.statespace.SARIMAX(\n",
    "    ads.Ads, order=(p, d, q), seasonal_order=(P, D, Q, s)\n",
    ").fit(disp=-1)\n",
    "print(best_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нека инспектираме остатъците от модела."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsplot(best_model.resid[24 + 1 :], lags=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ясно е, че остатъците са стационарни и няма видими автокорелации. Нека направим прогнози, използвайки нашия модел.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def plotSARIMA(series, model, n_steps):\n",
    "    \"\"\"\n",
    "        Plots model vs predicted values\n",
    "        \n",
    "        series - dataset with timeseries\n",
    "        model - fitted SARIMA model\n",
    "        n_steps - number of steps to predict in the future\n",
    "        \n",
    "    \"\"\"\n",
    "    # adding model values\n",
    "    data = series.copy()\n",
    "    data.columns = [\"actual\"]\n",
    "    data[\"arima_model\"] = model.fittedvalues\n",
    "    # making a shift on s+d steps, because these values were unobserved by the model\n",
    "    # due to the differentiating\n",
    "    data[\"arima_model\"][: s + d] = np.NaN\n",
    "\n",
    "    # forecasting on n_steps forward\n",
    "    forecast = model.predict(start=data.shape[0], end=data.shape[0] + n_steps)\n",
    "    forecast = data.arima_model.append(forecast)\n",
    "    # calculate error, again having shifted on s+d steps from the beginning\n",
    "    error = mean_absolute_percentage_error(\n",
    "        data[\"actual\"][s + d :], data[\"arima_model\"][s + d :]\n",
    "    )\n",
    "\n",
    "    plt.figure(figsize=(15, 7))\n",
    "    plt.title(\"Mean Absolute Percentage Error: {0:.2f}%\".format(error))\n",
    "    plt.plot(forecast, color=\"r\", label=\"model\")\n",
    "    plt.axvspan(data.index[-1], forecast.index[-1], alpha=0.5, color=\"lightgrey\")\n",
    "    plt.plot(data.actual, label=\"actual\")\n",
    "    plt.legend()\n",
    "    plt.grid(True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotSARIMA(ads, best_model, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В крайна сметка получихме много адекватни прогнози. Нашият модел греши средно с 4,01%, което е много, много добре. Въпреки това, общите разходи за подготовка на данни, правене на сериите неподвижни и избор на параметри може да не си струват тази точност.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Линейни (и не съвсем) модели за времеви редове\n",
    "\n",
    "Често в работата си трябва да създавам модели с [*бързо, добро, евтино*](http://fastgood.cheap) като единствен ръководен принцип. Това означава, че някои от тези модели никога няма да се считат за „готови за производство“, тъй като изискват твърде много време за подготовка на данните (както в SARIMA) или изискват често повторно обучение за нови данни (отново SARIMA) или са трудни за настройка (добре пример - SARIMA). Следователно много често е много по-лесно да изберете няколко функции от съществуващите времеви редове и да изградите прост линеен регресионен модел или, да речем, произволна гора. Добър е и евтин.\n",
    "\n",
    "Този подход не е подкрепен от теория и нарушава няколко предположения (напр. теоремата на Гаус-Марков, особено за грешките, които не са корелирани), но е много полезен на практика и често се използва в състезания по машинно обучение."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Извличане на функции"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Моделът се нуждае от функции и всичко, което имаме, е едномерен времеви ред. Какви функции можем да извлечем?\n",
    "* Изоставане на времеви редове\n",
    "* Статистика на прозореца:\n",
    "    - Макс./мин. стойност на серията в прозорец\n",
    "    - Средна/средна стойност в прозорец\n",
    "    - Дисперсия на прозореца\n",
    "    - и т.н.\n",
    "* Функции за дата и час:\n",
    "    - Минута от час, час от ден, ден от седмицата и т.н\n",
    "    - Този ден празник ли е? Може би има специално събитие? Представете това като булева характеристика\n",
    "* Целево кодиране\n",
    "* Прогнози от други модели (имайте предвид, че по този начин можем да загубим скоростта на прогнозиране)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нека прегледаме някои от методите и да видим какво можем да извлечем от нашите данни за времевите серии на рекламите.\n",
    "\n",
    "## Изоставане на времеви редове\n",
    "\n",
    "Премествайки серията $n$ със стъпки назад, получаваме колона с характеристики, където текущата стойност на времевата серия е подравнена със стойността й в момент $t-n$. Ако направим едно изместване на изоставането и обучим модел на тази характеристика, моделът ще може да прогнозира с 1 стъпка напред, след като е наблюдавал текущото състояние на серията. Увеличаването на забавянето, да речем, до 6, ще позволи на модела да прави прогнози с 6 стъпки напред; обаче ще използва данни, наблюдавани 6 стъпки назад. Ако нещо фундаментално промени серията през този ненаблюдаван период, моделът няма да улови тези промени и ще върне прогнози с голяма грешка. Следователно, по време на първоначалния избор на забавяне, трябва да се намери баланс между оптималното качество на прогнозата и дължината на прогнозния хоризонт."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Създаване на копие на първоначалната дейтаграма за извършване на различни трансформации\n",
    "data = pd.DataFrame(ads.Ads.copy())\n",
    "data.columns = [\"y\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Добавяне на изоставането на целевата променлива от 6 стъпки назад до 24\n",
    "for i in range(6, 25):\n",
    "    data[\"lag_{}\".format(i)] = data.y.shift(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# погледнете новата рамка с данни\n",
    "data.tail(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Страхотно, генерирахме набор от данни тук. Защо сега не обучим модел?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# за кръстосано валидиране на времеви серии, задайте 5 пъти\n",
    "tscv = TimeSeriesSplit(n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeseries_train_test_split(X, y, test_size):\n",
    "    \n",
    "\"\"\"\n",
    "        Извършете разделяне на train-test по отношение на структурата на времевите серии\n",
    "    \"\"\"\n",
    "\n",
    "    # вземете индекса, след който започва тестовият набор\n",
    "    test_index = int(len(X) * (1 - test_size))\n",
    "\n",
    "    X_train = X.iloc[:test_index]\n",
    "    y_train = y.iloc[:test_index]\n",
    "    X_test = X.iloc[test_index:]\n",
    "    y_test = y.iloc[test_index:]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data.dropna().y\n",
    "X = data.dropna().drop([\"y\"], axis=1)\n",
    "\n",
    "# запазване на 30% от данните за тестване\n",
    "X_train, X_test, y_train, y_test = timeseries_train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# машинно обучение в два реда\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     38
    ]
   },
   "outputs": [],
   "source": [
    "def plotModelResults(\n",
    "    model, X_train=X_train, X_test=X_test, plot_intervals=False, plot_anomalies=False\n",
    "):\n",
    "    \n",
    "\"\"\"\n",
    "        Графики, моделирани срещу фактически стойности, прогнозни интервали и аномалии\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    prediction = model.predict(X_test)\n",
    "\n",
    "    plt.figure(figsize=(15, 7))\n",
    "    plt.plot(prediction, \"g\", label=\"prediction\", linewidth=2.0)\n",
    "    plt.plot(y_test.values, label=\"actual\", linewidth=2.0)\n",
    "\n",
    "    if plot_intervals:\n",
    "        cv = cross_val_score(\n",
    "            model, X_train, y_train, cv=tscv, scoring=\"neg_mean_absolute_error\"\n",
    "        )\n",
    "        mae = cv.mean() * (-1)\n",
    "        deviation = cv.std()\n",
    "\n",
    "        scale = 1.96\n",
    "        lower = prediction - (mae + scale * deviation)\n",
    "        upper = prediction + (mae + scale * deviation)\n",
    "\n",
    "        plt.plot(lower, \"r--\", label=\"upper bond / lower bond\", alpha=0.5)\n",
    "        plt.plot(upper, \"r--\", alpha=0.5)\n",
    "\n",
    "        if plot_anomalies:\n",
    "            anomalies = np.array([np.NaN] * len(y_test))\n",
    "            anomalies[y_test < lower] = y_test[y_test < lower]\n",
    "            anomalies[y_test > upper] = y_test[y_test > upper]\n",
    "            plt.plot(anomalies, \"o\", markersize=10, label=\"Anomalies\")\n",
    "\n",
    "    error = mean_absolute_percentage_error(prediction, y_test)\n",
    "    plt.title(\"Mean absolute percentage error {0:.2f}%\".format(error))\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.tight_layout()\n",
    "    plt.grid(True)\n",
    "\n",
    "\n",
    "def plotCoefficients(model):\n",
    "    \"\"\"\n",
    "        Изобразява сортирани стойности на коефициента на модела\n",
    "    \"\"\"\n",
    "\n",
    "    coefs = pd.DataFrame(model.coef_, X_train.columns)\n",
    "    coefs.columns = [\"coef\"]\n",
    "    coefs[\"abs\"] = coefs.coef.apply(np.abs)\n",
    "    coefs = coefs.sort_values(by=\"abs\", ascending=False).drop([\"abs\"], axis=1)\n",
    "\n",
    "    plt.figure(figsize=(15, 7))\n",
    "    coefs.coef.plot(kind=\"bar\")\n",
    "    plt.grid(True, axis=\"y\")\n",
    "    plt.hlines(y=0, xmin=0, xmax=len(coefs), linestyles=\"dashed\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotModelResults(lr, plot_intervals=True)\n",
    "plotCoefficients(lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обикновените забавяния и линейната регресия ни дадоха прогнози, които не са толкова далеч от SARIMA по отношение на качеството. Има много ненужни функции, така че ще направим избор на функции след малко. Засега нека продължим с инженерството!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ще добавим час, ден от седмицата и булева стойност за „is_weekend“. За да направим това, трябва да трансформираме текущия индекс на кадъра с данни във формат „datetime“ и да извлечем „hour“ и „weekday“.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.index = pd.to_datetime(data.index)\n",
    "data[\"hour\"] = data.index.hour\n",
    "data[\"weekday\"] = data.index.weekday\n",
    "data[\"is_weekend\"] = data.weekday.isin([5, 6]) * 1\n",
    "data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можем да визуализираме получените характеристики."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 5))\n",
    "plt.title(\"Encoded features\")\n",
    "data.hour.plot()\n",
    "data.weekday.plot()\n",
    "data.is_weekend.plot()\n",
    "plt.grid(True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тъй като сега имаме различни скали в нашите променливи, хиляди за характеристиките на изоставането и десетки за категоричните, трябва да ги трансформираме в една и съща скала за изследване на важността на характеристиките и, по-късно, регулиране.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data.dropna().y\n",
    "X = data.dropna().drop([\"y\"], axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = timeseries_train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train_scaled, y_train)\n",
    "\n",
    "plotModelResults(lr, X_train=X_train_scaled, X_test=X_test_scaled, plot_intervals=True)\n",
    "plotCoefficients(lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Грешката на теста намалява малко. Съдейки по графиката на коефициентите, можем да кажем, че `weekday` и `is_weekend` са полезни функции.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Целево кодиране\n",
    "Бих искал да добавя още един вариант за кодиране на категорични променливи: кодиране по средна стойност. Ако е нежелателно да се разрушава набор от данни чрез използване на много фиктивни променливи, които могат да доведат до загуба на информация и ако те не могат да се използват като реални стойности поради конфликти като „0 часа < 23 часа“, тогава е възможно да се кодира променлива с малко по-интерпретируеми стойности. Естествената идея е да се кодира със средната стойност на целевата променлива. В нашия пример всеки ден от седмицата и всеки час от деня могат да бъдат кодирани от съответния среден брой реклами, гледани през този ден или час. Много е важно да се уверите, че средната стойност се изчислява само върху набора за обучение (или само върху текущото сгъване за кръстосано валидиране), така че моделът да не е наясно с бъдещето."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def code_mean(data, cat_feature, real_feature):\n",
    "    \"\"\"\n",
    "    Връща речник, където ключовете са уникални категории на cat_feature,\n",
    "    и стойностите са средни стойности над real_feature\n",
    "    \"\"\"\n",
    "    return dict(data.groupby(cat_feature)[real_feature].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нека да разгледаме средните стойности по часове."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_hour = code_mean(data, \"hour\", \"y\")\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.title(\"Hour averages\")\n",
    "pd.DataFrame.from_dict(average_hour, orient=\"index\")[0].plot()\n",
    "plt.grid(True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И накрая, нека съберем всички трансформации в една функция."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def prepareData(series, lag_start, lag_end, test_size, target_encoding=False):\n",
    "    \"\"\"\n",
    "        серия: pd.DataFrame\n",
    "            рамка от данни с времеви серии\n",
    "\n",
    "        lag_start: int\n",
    "            първоначална стъпка назад във времето за разделяне на целевата променлива\n",
    "            пример - lag_start = 1 означава, че моделът\n",
    "                      ще видите вчерашните стойности, за да прогнозирате днес\n",
    "\n",
    "        lag_end: int\n",
    "            последна стъпка назад във времето за разделяне на целевата променлива\n",
    "            пример - lag_end = 4 означава, че моделът\n",
    "                      ще видите до 4 дни назад във времето, за да прогнозирате днес\n",
    "\n",
    "        test_size: float\n",
    "            размер на тестовия набор от данни след разделянето на влак/тест като процент от набора от данни\n",
    "\n",
    "        target_encoding: boolean\n",
    "            ако е True - добавете целеви средни стойности към набора от данни\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    # копие на първоначалния набор от данни\n",
    "    data = pd.DataFrame(series.copy())\n",
    "    data.columns = [\"y\"]\n",
    "\n",
    "    # закъснения на серия\n",
    "    for i in range(lag_start, lag_end):\n",
    "        data[\"lag_{}\".format(i)] = data.y.shift(i)\n",
    "\n",
    "    # datetime features\n",
    "    data.index = pd.to_datetime(data.index)\n",
    "    data[\"hour\"] = data.index.hour\n",
    "    data[\"weekday\"] = data.index.weekday\n",
    "    data[\"is_weekend\"] = data.weekday.isin([5, 6]) * 1\n",
    "\n",
    "    if target_encoding:\n",
    "    # изчисляване на средни стойности само за train\n",
    "        test_index = int(len(data.dropna()) * (1 - test_size))\n",
    "        data[\"weekday_average\"] = list(\n",
    "            map(code_mean(data[:test_index], \"weekday\", \"y\").get, data.weekday)\n",
    "        )\n",
    "        data[\"hour_average\"] = list(\n",
    "            map(code_mean(data[:test_index], \"hour\", \"y\").get, data.hour)\n",
    "        )\n",
    "\n",
    "    # капка кодирани променливи\n",
    "        data.drop([\"hour\", \"weekday\"], axis=1, inplace=True)\n",
    "\n",
    "    # train-test split\n",
    "    y = data.dropna().y\n",
    "    X = data.dropna().drop([\"y\"], axis=1)\n",
    "    X_train, X_test, y_train, y_test = timeseries_train_test_split(\n",
    "        X, y, test_size=test_size\n",
    "    )\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = prepareData(\n",
    "    ads.Ads, lag_start=6, lag_end=25, test_size=0.3, target_encoding=True\n",
    ")\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train_scaled, y_train)\n",
    "\n",
    "plotModelResults(\n",
    "    lr,\n",
    "    X_train=X_train_scaled,\n",
    "    X_test=X_test_scaled,\n",
    "    plot_intervals=True,\n",
    "    plot_anomalies=True,\n",
    ")\n",
    "plotCoefficients(lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Виждаме някакво **прекомерно оборудване**! „Hour_average“ беше толкова голямо в обучителния набор от данни, че моделът реши да концентрира всичките си сили върху него. В резултат на това качеството на прогнозата падна. Този проблем може да бъде разрешен по различни начини; например можем да изчислим целевото кодиране не за целия набор от влакове, а вместо това за някакъв прозорец. По този начин кодировките от последния наблюдаван прозорец най-вероятно ще опишат по-добре текущото състояние на серията. Като алтернатива можем просто да го пуснем ръчно, тъй като сме сигурни, че това само влошава нещата в този случай.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = prepareData(\n",
    "    ads.Ads, lag_start=6, lag_end=25, test_size=0.3, target_encoding=False\n",
    ")\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Регулиране и избор на функции\n",
    "\n",
    "Както вече знаем, не всички функции са еднакво здрави - някои могат да доведат до прекомерно оборудване, докато други трябва да бъдат премахнати. Освен ръчна проверка, можем да приложим регуляризация. Два от най-популярните регресионни модели с регуляризация са регресиите на Ridge и Lasso. И двете добавят някои допълнителни ограничения към нашата функция за загуба.\n",
    "\n",
    "В случай на регресия на Ридж, тези ограничения са сумата от квадратите на коефициентите, умножени по коефициента на регулация. Колкото по-голям е коефициентът, толкова по-голяма ще бъде загубата ни. Следователно ще се опитаме да оптимизираме модела, като запазим коефициентите сравнително ниски.\n",
    "\n",
    "В резултат на тази $L2$ регулация ще имаме по-голямо отклонение и по-ниска вариация, така че моделът ще обобщава по-добре (поне това е, което се надяваме да се случи).\n",
    "\n",
    "Вторият регресионен модел, Lasso regression, добавя към функцията на загубите не квадрати, а абсолютни стойности на коефициентите. В резултат на това по време на процеса на оптимизация коефициентите на маловажни характеристики могат да станат нули, което позволява автоматизиран избор на характеристики. Този тип регулация се нарича $L1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Първо, нека се уверим, че имаме функции за премахване и че данните имат силно корелирани характеристики.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(X_train.corr());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV, RidgeCV\n",
    "\n",
    "ridge = RidgeCV(cv=tscv)\n",
    "ridge.fit(X_train_scaled, y_train)\n",
    "\n",
    "plotModelResults(\n",
    "    ridge,\n",
    "    X_train=X_train_scaled,\n",
    "    X_test=X_test_scaled,\n",
    "    plot_intervals=True,\n",
    "    plot_anomalies=True,\n",
    ")\n",
    "plotCoefficients(ridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можем ясно да видим, че някои коефициенти се доближават все повече и повече до нула (въпреки че всъщност никога не я достигат), тъй като значението им в модела пада.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = LassoCV(cv=tscv)\n",
    "lasso.fit(X_train_scaled, y_train)\n",
    "\n",
    "plotModelResults(\n",
    "    lasso,\n",
    "    X_train=X_train_scaled,\n",
    "    X_test=X_test_scaled,\n",
    "    plot_intervals=True,\n",
    "    plot_anomalies=True,\n",
    ")\n",
    "plotCoefficients(lasso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ласо регресията се оказа по-консервативна; премахна 23-то изоставане от най-важните функции и напълно премахна 5 характеристики, което само подобри качеството на прогнозиране."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подсилване\n",
    "Защо не трябва да опитаме XGBoost сега?\n",
    "<img src=\"../../img/xgboost_the_things.jpg\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     3
    ]
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "xgb = XGBRegressor(verbosity=0)\n",
    "xgb.fit(X_train_scaled, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotModelResults(\n",
    "    xgb,\n",
    "    X_train=X_train_scaled,\n",
    "    X_test=X_test_scaled,\n",
    "    plot_intervals=True,\n",
    "    plot_anomalies=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Имаме победител! Това е най-малката грешка в тестовата серия сред всички модели, които сме пробвали досега.\n",
    "\n",
    "Но тази победа е решаваща и може би не е най-добрата идея да поставите „xgboost“ веднага щом се докопате до данни от времеви серии. Като цяло, дървовидните модели се справят лошо с тенденциите в данните в сравнение с линейните модели. В такъв случай първо ще трябва да премахнете тренда на сериала си или да използвате някои трикове, за да направите магията да се случи. В идеалния случай можете да направите серията неподвижна и след това да използвате XGBoost. Например, можете да прогнозирате тенденция отделно с линеен модел и след това да добавите прогнози от „xgboost“, за да получите окончателна прогноза."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Заключение\n",
    "\n",
    "Обсъдихме различни методи за анализ на времеви редове и прогнози. За съжаление или може би за щастие, няма един единствен начин за разрешаване на този тип проблеми. Методите, разработени през 60-те години (а някои дори в началото на 21-ви век), са все още популярни, заедно с LSTM и RNN (не са обхванати в тази статия). Това е отчасти свързано с факта, че задачата за прогнозиране, както всяка друга задача, свързана с данни, изисква креативност в толкова много аспекти и определено изисква проучване. Въпреки големия брой формални показатели за качество и подходи за оценка на параметрите, често е необходимо да се опита нещо различно за всеки времеви ред. Не на последно място важен е балансът между качество и цена. Като добър пример, моделът SARIMA може да доведе до впечатляващи резултати след настройка, но може да изисква много часове манипулиране на времеви серии ~~танци на тамбура~~, докато прост линеен регресионен модел може да бъде изграден за 10 минути и може да постигне повече или по-малко сравними резултати."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Полезни ресурси\n",
    "\n",
    "* Същият бележник като интерактивен уеб базиран [Kaggle Kernel](https://www.kaggle.com/kashnitsky/topic-9-part-1-time-series-analysis-in-python)\n",
    "* „LSTM (Long Short Term Memory) Networks for predicting Time Series“ – урок от Макс Сергей Булаев в mlcourse.ai (пълният списък с уроци е [тук](https://mlcourse.ai/tutorials))\n",
    "* Основно ястие [сайт](https://mlcourse.ai), [репо за курс](https://github.com/Yorko/mlcourse.ai) и YouTube [канал](https://www.youtube. com/watch?v=QKTuw4PNOsU&list=PLVlY_7IJCMJeRfZ68eVfEcu-UcN9BbwiX)\n",
    "* Материали за курса като [Набор от данни на Kaggle](https://www.kaggle.com/kashnitsky/mlcourse)\n",
    "* Среден [\"история\"](https://medium.com/open-machine-learning-course/open-machine-learning-course-topic-9-time-series-analysis-in-python-a270cb05e0b3?source= collection_home---6------2--------------------) въз основа на този бележник\n",
    "* Ако четете руски: [статия](https://habr.com/ru/company/ods/blog/327242/) на Habr.com със ~ същия материал. И [лекция](https://youtu.be/_9lBwXnbOd8) в YouTube\n",
    "* [Онлайн учебник](https://people.duke.edu/~rnau/411home.htm) за курса за разширено статистическо прогнозиране в университета Дюк - обхваща подробно различни техники за изглаждане заедно с линейни и ARIMA модели\n",
    "* [Сравнение на моделите на времевите серии ARIMA и Random Forest за прогнозиране на огнища на птичи грип H5N1] (https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-15-276) - един от малкото случаи, при които се използва случайна гора за прогнозиране на времеви редове се защитава активно\n",
    "* [Анализ на времеви серии (TSA) в Python - линейни модели към GARCH](http://www.blackarbs.com/blog/time-series-analysis-in-python-linear-models-to-garch/11/1 /2016) - прилагане на фамилията модели ARIMA към задачата за моделиране на финансови показатели (от Браян Кристофър)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
