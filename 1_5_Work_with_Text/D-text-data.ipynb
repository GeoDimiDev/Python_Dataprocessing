{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../data/UniBIT_Logo_5.jpeg\">\n",
    "<center>\n",
    "\n",
    "# **Анализ и обработка на данни с Python**\n",
    "\n",
    "<img src=\"../data/Lecture_Logo_2.webp\">\n",
    "    \n",
    "## **УниБИТ**\n",
    "\n",
    "</center>\n",
    "Лекциите са базирани на публикуван в kaggle материал - https://www.kaggle.com/code/prashant111/a-beginners-guide-to-dealing-with-text-data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"0\"></a>\n",
    "# **Work with Text Data**\n",
    "\n",
    "# **Работа с текстови данни**\n",
    "\n",
    "Тук са изложени някои често срещани начина за работа с текстови данни. Ще обсъдим различни методи за извличане на признаци. Ще започнем с някои основни техники, които ще доведат до напреднали техники за обработка на естествен език. Ще научим и за предварителната обработка на текстовите данни, за да извлечем по-добри характеристики от чисти данни."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"0.1\"></a>\n",
    "# **Table of Contents** \n",
    "\n",
    "1.\t[Introduction to Natural Language Processing](#1)\n",
    "2.\t[Basic feature extraction using text data](#2)\n",
    "   - 2.1 [Count number of words](#2.1)\n",
    "   - 2.2 [Count number of characters](#2.2)\n",
    "   - 2.3 [Average word length](#2.3)\n",
    "   - 2.4 [Number of stopwords](#2.4)\n",
    "   - 2.5 [Number of special characters](#2.5)\n",
    "   - 2.6 [Number of numerics](#2.6)\n",
    "   - 2.7 [Number of uppercase words](#2.7)\n",
    "3.\t[Basic Text Pre-processing of text data](#3)\n",
    "   - 3.1 [CountVectorization](#3.1)\n",
    "   - 3.2 [HashingVectorizer](#3.2)\n",
    "   - 3.3 [Lower casing](#3.3)\n",
    "   - 3.4 [Punctuation removal](#3.4)\n",
    "   - 3.5 [Stopwords removal](#3.5)\n",
    "   - 3.6 [Frequent words removal](#3.6)\n",
    "   - 3.7 [Rare words removal](#3.7)\n",
    "   - 3.8 [Spelling correction](#3.8)\n",
    "   - 3.9 [Tokenization](#3.9)\n",
    "   - 3.10 [Stemming](#3.10)\n",
    "   - 3.11 [Lemmatization](#3.11)\n",
    "4.\t[Advance Text Processing](#4)\n",
    "   - 4.1 [N-grams](#4.1)\n",
    "   - 4.2 [Term Frequency](#4.2)\n",
    "   - 4.3 [Inverse Document Frequency](#4.3)\n",
    "   - 4.4 [Term Frequency-Inverse Document Frequency (TF-IDF)](#4.4)\n",
    "   - 4.5 [Bag of Words](#4.5)\n",
    "   - 4.6 [Sentiment Analysis](#4.6)\n",
    "5.  [References](#5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1. Introduction to Natural Language Processing** <a class=\"anchor\" id=\"1\"></a>\n",
    "\n",
    "\n",
    "[Съдържание](#0.1)\n",
    "\n",
    "\n",
    "Ние често се сблъскваме с термина **Natural Language Processing (NLP)**- или **Натурална Езикова Обработка (НЕО)**. Но замисляли ли сте се някога какво разбираме под обработка на естествен език. Накратко, NLP е изкуството да се разбира значението и влиянието на думите - \n",
    "\n",
    "\n",
    "[Обработката на естествен език](https://en.wikipedia.org/wiki/Natural_language_processing) е подобласт на лингвистиката, компютърните науки, информационното инженерство и изкуствения интелект, която се занимава с взаимодействието между компютрите и човешките (естествените) езици, по-специално как да се програмират компютри за обработка и анализ на големи количества данни от естествен език.\n",
    "\n",
    "Предизвикателствата при обработката на естествен език често включват [разпознаване на реч](https://en.wikipedia.org/wiki/Speech_recognition), [разбиране на естествен език](https://en.wikipedia.org/wiki/Natural-language_understanding) и [генериране на естествен език](https://en.wikipedia.org/wiki/Natural-language_generation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "source": [
    "Някои от най-полезните NLP техники, използващи се в практиката са:\n",
    "\n",
    "- **CountVectorization**, \n",
    "- **Hashing Vectorization**, \n",
    "- **Term Frequency-Inverse Document Frequency (TF-IDF)**, \n",
    "- **Lemmatization**, \n",
    "- **Stemming**, \n",
    "- **Parsing**, and \n",
    "- **Sentiment Analysis**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2. Извличане на характеристики от текстови данни** <a class=\"anchor\" id=\"2\"></a>\n",
    "\n",
    "\n",
    "[Съдържание](#0.1)\n",
    "\n",
    "\n",
    "- Да предположим, че нямаме достатъчно познания по обработка на естествен език.\n",
    "\n",
    "- Все пак можем да използваме някои основни техники за извличане на признаци, за да извлечем признаци от текстови данни.\n",
    "\n",
    "- В този раздел ще разгледаме някои основни техники за извличане на признаци.\n",
    "\n",
    "- Но първо нека импортираме данните.\n",
    "\n",
    "- За тази тетрадка използвахме данните от [Twitter Sentiment Analysis](https://www.kaggle.com/arkhoshghalb/twitter-sentiment-analysis-hatred-speech)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Load necessary libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Тази среда на Python 3 идва с много, предварително инсталирани, полезни библиотеки за анализ\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# Например, ето няколко полезни пакета за зареждане \n",
    "\n",
    "import numpy as np # линейна алгебра\n",
    "import pandas as pd # обработка на данни, CSV файл I/O (напр. pd.read_csv)\n",
    "\n",
    "# Файловете с входящи данни са налични в директорията \"../input/\".\n",
    "# Например, стартирането на това (чрез щракване върху \"run\" или натискане на Shift+Enter) ще изведе всички файлове във входящата директория\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **Прочит на dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 14.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#train = pd.read_csv('../data/train_text.csv')\n",
    "train = pd.read_csv('../data/Train_Smirnenski.csv')\n",
    "test = pd.read_csv('../data/Test_Smirnenski.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Преглед на dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Христо Смирненски</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Да бъде ден</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Том първи. Стихотворения</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>I. Да бъде ден!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                      Text\n",
       "0   0         Христо Смирненски\n",
       "1   1               Да бъде ден\n",
       "2   2  Том първи. Стихотворения\n",
       "3   3                       NaN\n",
       "4   4           I. Да бъде ден!"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID              int64\n",
       "Text           object\n",
       "word_count      int64\n",
       "char_count    float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Христо Смирненски</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Прости му, господи!*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[* _Публикация:_ Българан. Седмично хумористич...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                                               Text\n",
       "0   0                                  Христо Смирненски\n",
       "1   1                                                NaN\n",
       "2   2                               Прости му, господи!*\n",
       "3   3                                                NaN\n",
       "4   4  [* _Публикация:_ Българан. Седмично хумористич..."
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID              int64\n",
       "Text           object\n",
       "word_count      int64\n",
       "char_count    float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **2.1 Преброяване на думи** <a class=\"anchor\" id=\"2.1\"></a>\n",
    "\n",
    "\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "\n",
    "- Едно от най-основните изисквания в НЛП анализа е да преброите броя на думите във всеки tweet. Идеята зад това е, че          **негативните коментари съдържат по-малко думи от положителните**.\n",
    "\n",
    "- Можем да изпълним горната задача (да преброим броя на думите), като използваме функцията **split** в python по следния начин-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_of_words(df):\n",
    "    df['word_count'] = df['Text'].apply(lambda x : len(str(x).split(\" \")))\n",
    "    print(df[['Text','word_count']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       Text  word_count\n",
      "0         Христо Смирненски           2\n",
      "1               Да бъде ден           3\n",
      "2  Том първи. Стихотворения           3\n",
      "3                       NaN           1\n",
      "4           I. Да бъде ден!           4\n"
     ]
    }
   ],
   "source": [
    "num_of_words(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Text  word_count\n",
      "0                                  Христо Смирненски           2\n",
      "1                                                NaN           1\n",
      "2                               Прости му, господи!*           3\n",
      "3                                                NaN           1\n",
      "4  [* _Публикация:_ Българан. Седмично хумористич...          19\n"
     ]
    }
   ],
   "source": [
    "num_of_words(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Виждаме, че броят на думите във всеки туит е изчислен по-горе."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  **2.2 Преброяване на броя знаци**  <a class=\"anchor\" id=\"2.2\"></a>\n",
    "\n",
    "\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "\n",
    "- Можем също така да изчислим броя знаци във всеки tweet. Начина е много подобен на предния.\n",
    "\n",
    "- Това може да се постигне чрез изчисляване на дължината на tweet-а по следния начин -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def num_of_chars(df):\n",
    "    df['char_count'] = df['Text'].str.len() ## това също включва интервали\n",
    "    print(df[['Text','char_count']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       Text  char_count\n",
      "0         Христо Смирненски        17.0\n",
      "1               Да бъде ден        11.0\n",
      "2  Том първи. Стихотворения        24.0\n",
      "3                       NaN         NaN\n",
      "4           I. Да бъде ден!        15.0\n"
     ]
    }
   ],
   "source": [
    "num_of_chars(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Text  char_count\n",
      "0                                  Христо Смирненски        17.0\n",
      "1                                                NaN         NaN\n",
      "2                               Прости му, господи!*        20.0\n",
      "3                                                NaN         NaN\n",
      "4  [* _Публикация:_ Българан. Седмично хумористич...       122.0\n"
     ]
    }
   ],
   "source": [
    "num_of_chars(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Можем да видим, че броят на символите във всеки tweet е изчислен по-горе.\n",
    "\n",
    "- Горното изчисление ще включва и броя на интервалите, които можем да премахнем, ако е необходимо."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.3 Средна дължина на думата** <a class=\"anchor\" id=\"2.3\"></a>\n",
    "\n",
    "\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "\n",
    "- Сега броят на думите и броят на знаците са важни. Но има друга особеност, която също е важна, е **средната дължина на думата** на всеки туит. Тази функция може да ни помогне да подобрим нашия модел.\n",
    "\n",
    "- Можем да изпълним горната задача, като просто вземем сумата от дължината на всички думи и я разделим на общата дължина на туита."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def avg_word(sentence):\n",
    "    words = sentence.split()    \n",
    "    return (sum(len(word) for word in words)/len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def avg_word_length(df):\n",
    "    df['avg_word'] = df['Text'].apply(lambda x: avg_word(x))\n",
    "    print(df[['Text','avg_word']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m avg_word_length(train)\n",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m, in \u001b[0;36mavg_word_length\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mavg_word_length\u001b[39m(df):\n\u001b[0;32m----> 2\u001b[0m     df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mavg_word\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mText\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: avg_word(x))\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mText\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mavg_word\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mhead())\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/series.py:4630\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4520\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4521\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4522\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4525\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4526\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4527\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4528\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4529\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4628\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4629\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4630\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SeriesApply(\u001b[38;5;28mself\u001b[39m, func, convert_dtype, args, kwargs)\u001b[38;5;241m.\u001b[39mapply()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/apply.py:1025\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;66;03m# self.f is Callable\u001b[39;00m\n\u001b[0;32m-> 1025\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_standard()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/apply.py:1076\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1074\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1075\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[0;32m-> 1076\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mmap_infer(\n\u001b[1;32m   1077\u001b[0m             values,\n\u001b[1;32m   1078\u001b[0m             f,\n\u001b[1;32m   1079\u001b[0m             convert\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_dtype,\n\u001b[1;32m   1080\u001b[0m         )\n\u001b[1;32m   1082\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1083\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1085\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/_libs/lib.pyx:2834\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m, in \u001b[0;36mavg_word_length.<locals>.<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mavg_word_length\u001b[39m(df):\n\u001b[0;32m----> 2\u001b[0m     df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mavg_word\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mText\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: avg_word(x))\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mText\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mavg_word\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mhead())\n",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m, in \u001b[0;36mavg_word\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mavg_word\u001b[39m(sentence):\n\u001b[0;32m----> 2\u001b[0m     words \u001b[38;5;241m=\u001b[39m sentence\u001b[38;5;241m.\u001b[39msplit()    \n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mlen\u001b[39m(word) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words)\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(words))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "avg_word_length(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[123], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m avg_word_length(test)\n",
      "Cell \u001b[1;32mIn[120], line 2\u001b[0m, in \u001b[0;36mavg_word_length\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mavg_word_length\u001b[39m(df):\n\u001b[1;32m----> 2\u001b[0m     df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mavg_word\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mText\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: avg_word(x))\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mText\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mavg_word\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mhead())\n",
      "File \u001b[1;32m~\\.conda\\envs\\Work_Py_4\\Lib\\site-packages\\pandas\\core\\series.py:4630\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4520\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4521\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4522\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4525\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4526\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4527\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4528\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4529\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4628\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4629\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4630\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SeriesApply(\u001b[38;5;28mself\u001b[39m, func, convert_dtype, args, kwargs)\u001b[38;5;241m.\u001b[39mapply()\n",
      "File \u001b[1;32m~\\.conda\\envs\\Work_Py_4\\Lib\\site-packages\\pandas\\core\\apply.py:1025\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[0;32m   1024\u001b[0m \u001b[38;5;66;03m# self.f is Callable\u001b[39;00m\n\u001b[1;32m-> 1025\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_standard()\n",
      "File \u001b[1;32m~\\.conda\\envs\\Work_Py_4\\Lib\\site-packages\\pandas\\core\\apply.py:1076\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1074\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1075\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[1;32m-> 1076\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mmap_infer(\n\u001b[0;32m   1077\u001b[0m             values,\n\u001b[0;32m   1078\u001b[0m             f,\n\u001b[0;32m   1079\u001b[0m             convert\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_dtype,\n\u001b[0;32m   1080\u001b[0m         )\n\u001b[0;32m   1082\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1083\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1084\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1085\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32m~\\.conda\\envs\\Work_Py_4\\Lib\\site-packages\\pandas\\_libs\\lib.pyx:2834\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[120], line 2\u001b[0m, in \u001b[0;36mavg_word_length.<locals>.<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mavg_word_length\u001b[39m(df):\n\u001b[1;32m----> 2\u001b[0m     df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mavg_word\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mText\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: avg_word(x))\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mText\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mavg_word\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mhead())\n",
      "Cell \u001b[1;32mIn[119], line 2\u001b[0m, in \u001b[0;36mavg_word\u001b[1;34m(sentence)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mavg_word\u001b[39m(sentence):\n\u001b[1;32m----> 2\u001b[0m     words \u001b[38;5;241m=\u001b[39m sentence\u001b[38;5;241m.\u001b[39msplit()    \n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mlen\u001b[39m(word) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words)\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(words))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'float' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "avg_word_length(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.4 Брой стоп думи** <a class=\"anchor\" id=\"2.4\"></a>\n",
    "\n",
    "\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "\n",
    "- Като цяло, когато решаваме всеки НЛП проблем, първото нещо, което правим, е да премахнем стоп думите. Но какво са стоп думите?\n",
    "\n",
    "- **Stop words**: Стоп думата е често използвана дума като `the`, `a`, `an`, `in`, които се филтрират преди или след обработката на данни на естествен език (текст). Понякога изчисляването на броя на спиращите думи може също да ни даде допълнителна информация, която може да сме загубили преди.\n",
    "\n",
    "- За повече информация относно стоп думите, моля, посетете следните връзки-\n",
    "\n",
    "- https://en.wikipedia.org/wiki/Stop_words\n",
    "\n",
    "- https://www.geeksforgeeks.org/removing-stop-words-nltk-python/\n",
    "\n",
    "- https://kavita-ganesan.com/what-are-stop-words/#.XowrncgzbIU\n",
    "\n",
    "\n",
    "- За да проверим списъка със стоп думи, можем да напишем следните команди."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "#from nltk.corpus import stopwords\n",
    "#set(stopwords.words('english'))\n",
    "\n",
    "stopwords = pd.read_csv('../data/stopwords_bulgarian.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Можем да преброим броя на стоп думите, по следния начин-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from nltk.corpus import stopwords\n",
    "#stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_words(df):\n",
    "    df['stopwords'] = df['Text'].apply(lambda x: len([x for x in x.split() if x in stop]))\n",
    "    print(df[['Text','stopwords']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[127], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m stop_words(train)\n",
      "Cell \u001b[1;32mIn[126], line 2\u001b[0m, in \u001b[0;36mstop_words\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstop_words\u001b[39m(df):\n\u001b[1;32m----> 2\u001b[0m     df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstopwords\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mText\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mlen\u001b[39m([x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m x\u001b[38;5;241m.\u001b[39msplit() \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m stop]))\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mText\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstopwords\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mhead())\n",
      "File \u001b[1;32m~\\.conda\\envs\\Work_Py_4\\Lib\\site-packages\\pandas\\core\\series.py:4630\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4520\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4521\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4522\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4525\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4526\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4527\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4528\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4529\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4628\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4629\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4630\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SeriesApply(\u001b[38;5;28mself\u001b[39m, func, convert_dtype, args, kwargs)\u001b[38;5;241m.\u001b[39mapply()\n",
      "File \u001b[1;32m~\\.conda\\envs\\Work_Py_4\\Lib\\site-packages\\pandas\\core\\apply.py:1025\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[0;32m   1024\u001b[0m \u001b[38;5;66;03m# self.f is Callable\u001b[39;00m\n\u001b[1;32m-> 1025\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_standard()\n",
      "File \u001b[1;32m~\\.conda\\envs\\Work_Py_4\\Lib\\site-packages\\pandas\\core\\apply.py:1076\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1074\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1075\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[1;32m-> 1076\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mmap_infer(\n\u001b[0;32m   1077\u001b[0m             values,\n\u001b[0;32m   1078\u001b[0m             f,\n\u001b[0;32m   1079\u001b[0m             convert\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_dtype,\n\u001b[0;32m   1080\u001b[0m         )\n\u001b[0;32m   1082\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1083\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1084\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1085\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32m~\\.conda\\envs\\Work_Py_4\\Lib\\site-packages\\pandas\\_libs\\lib.pyx:2834\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[126], line 2\u001b[0m, in \u001b[0;36mstop_words.<locals>.<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstop_words\u001b[39m(df):\n\u001b[1;32m----> 2\u001b[0m     df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstopwords\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mText\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mlen\u001b[39m([x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m x\u001b[38;5;241m.\u001b[39msplit() \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m stop]))\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mText\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstopwords\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mhead())\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'float' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "stop_words(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[128], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m stop_words(test)\n",
      "Cell \u001b[1;32mIn[126], line 2\u001b[0m, in \u001b[0;36mstop_words\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstop_words\u001b[39m(df):\n\u001b[1;32m----> 2\u001b[0m     df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstopwords\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mText\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mlen\u001b[39m([x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m x\u001b[38;5;241m.\u001b[39msplit() \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m stop]))\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mText\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstopwords\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mhead())\n",
      "File \u001b[1;32m~\\.conda\\envs\\Work_Py_4\\Lib\\site-packages\\pandas\\core\\series.py:4630\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4520\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4521\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4522\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4525\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4526\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4527\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4528\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4529\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4628\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4629\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4630\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SeriesApply(\u001b[38;5;28mself\u001b[39m, func, convert_dtype, args, kwargs)\u001b[38;5;241m.\u001b[39mapply()\n",
      "File \u001b[1;32m~\\.conda\\envs\\Work_Py_4\\Lib\\site-packages\\pandas\\core\\apply.py:1025\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[0;32m   1024\u001b[0m \u001b[38;5;66;03m# self.f is Callable\u001b[39;00m\n\u001b[1;32m-> 1025\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_standard()\n",
      "File \u001b[1;32m~\\.conda\\envs\\Work_Py_4\\Lib\\site-packages\\pandas\\core\\apply.py:1076\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1074\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1075\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[1;32m-> 1076\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mmap_infer(\n\u001b[0;32m   1077\u001b[0m             values,\n\u001b[0;32m   1078\u001b[0m             f,\n\u001b[0;32m   1079\u001b[0m             convert\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_dtype,\n\u001b[0;32m   1080\u001b[0m         )\n\u001b[0;32m   1082\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1083\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1084\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1085\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32m~\\.conda\\envs\\Work_Py_4\\Lib\\site-packages\\pandas\\_libs\\lib.pyx:2834\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[126], line 2\u001b[0m, in \u001b[0;36mstop_words.<locals>.<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstop_words\u001b[39m(df):\n\u001b[1;32m----> 2\u001b[0m     df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstopwords\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mText\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mlen\u001b[39m([x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m x\u001b[38;5;241m.\u001b[39msplit() \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m stop]))\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mText\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstopwords\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mhead())\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'float' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "stop_words(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.5 Брой специални знаци** <a class=\"anchor\" id=\"2.5\"></a>\n",
    "\n",
    "\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "\n",
    "- Още една интересна функция, която можем да извлечем от един туит, е да изчислим броя на хаштаговете в него. Той също така помага при извличането на допълнителна информация от нашите текстови данни.\n",
    "\n",
    "- Тук използваме функцията „startswith“, тъй като хаштаговете винаги се появяват в началото на думата."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_tags(df):\n",
    "    df['hashtags'] = df['Text'].apply(lambda x: len([x for x in x.split() if x.startswith('#')]))\n",
    "    print(df[['Text','hashtags']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[130], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m hash_tags(train)\n",
      "Cell \u001b[1;32mIn[129], line 2\u001b[0m, in \u001b[0;36mhash_tags\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhash_tags\u001b[39m(df):\n\u001b[1;32m----> 2\u001b[0m     df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhashtags\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mText\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mlen\u001b[39m([x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m x\u001b[38;5;241m.\u001b[39msplit() \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m#\u001b[39m\u001b[38;5;124m'\u001b[39m)]))\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mText\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhashtags\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mhead())\n",
      "File \u001b[1;32m~\\.conda\\envs\\Work_Py_4\\Lib\\site-packages\\pandas\\core\\series.py:4630\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4520\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4521\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4522\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4525\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4526\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4527\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4528\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4529\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4628\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4629\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4630\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SeriesApply(\u001b[38;5;28mself\u001b[39m, func, convert_dtype, args, kwargs)\u001b[38;5;241m.\u001b[39mapply()\n",
      "File \u001b[1;32m~\\.conda\\envs\\Work_Py_4\\Lib\\site-packages\\pandas\\core\\apply.py:1025\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[0;32m   1024\u001b[0m \u001b[38;5;66;03m# self.f is Callable\u001b[39;00m\n\u001b[1;32m-> 1025\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_standard()\n",
      "File \u001b[1;32m~\\.conda\\envs\\Work_Py_4\\Lib\\site-packages\\pandas\\core\\apply.py:1076\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1074\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1075\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[1;32m-> 1076\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mmap_infer(\n\u001b[0;32m   1077\u001b[0m             values,\n\u001b[0;32m   1078\u001b[0m             f,\n\u001b[0;32m   1079\u001b[0m             convert\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_dtype,\n\u001b[0;32m   1080\u001b[0m         )\n\u001b[0;32m   1082\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1083\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1084\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1085\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32m~\\.conda\\envs\\Work_Py_4\\Lib\\site-packages\\pandas\\_libs\\lib.pyx:2834\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[129], line 2\u001b[0m, in \u001b[0;36mhash_tags.<locals>.<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhash_tags\u001b[39m(df):\n\u001b[1;32m----> 2\u001b[0m     df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhashtags\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mText\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mlen\u001b[39m([x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m x\u001b[38;5;241m.\u001b[39msplit() \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m#\u001b[39m\u001b[38;5;124m'\u001b[39m)]))\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mText\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhashtags\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mhead())\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'float' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "hash_tags(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               tweet  hashtags\n",
      "0  #studiolife #aislife #requires #passion #dedic...         7\n",
      "1   @user #white #supremacists want everyone to s...         4\n",
      "2  safe ways to heal your #acne!!    #altwaystohe...         4\n",
      "3  is the hp and the cursed child book up for res...         3\n",
      "4    3rd #bihday to my amazing, hilarious #nephew...         2\n"
     ]
    }
   ],
   "source": [
    "hash_tags(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.6 Брой числа** <a class=\"anchor\" id=\"2.6\"></a>\n",
    "\n",
    "\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "\n",
    "- Точно както изчислихме броя на думите, можем да изчислим и броя на числата, които присъстват в туитовете. Това е полезна функция, която трябва да се изпълнява, докато правите подобни упражнения. Например -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_numerics(df):\n",
    "    df['numerics'] = df['tweet'].apply(lambda x: len([x for x in x.split() if x.isdigit()]))\n",
    "    print(df[['tweet','numerics']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               tweet  numerics\n",
      "0   @user when a father is dysfunctional and is s...         0\n",
      "1  @user @user thanks for #lyft credit i can't us...         0\n",
      "2                                bihday your majesty         0\n",
      "3  #model   i love u take with u all the time in ...         0\n",
      "4             factsguide: society now    #motivation         0\n"
     ]
    }
   ],
   "source": [
    "num_numerics(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               tweet  numerics\n",
      "0  #studiolife #aislife #requires #passion #dedic...         0\n",
      "1   @user #white #supremacists want everyone to s...         0\n",
      "2  safe ways to heal your #acne!!    #altwaystohe...         0\n",
      "3  is the hp and the cursed child book up for res...         0\n",
      "4    3rd #bihday to my amazing, hilarious #nephew...         0\n"
     ]
    }
   ],
   "source": [
    "num_numerics(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.7 Брой думи с главни букви**  <a class=\"anchor\" id=\"2.7\"></a>\n",
    "\n",
    "\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "\n",
    "- Гневът или яростта често се изразяват чрез писане с ГЛАВНИ БУКВИ, което прави това необходима операция за идентифициране на тези думи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_uppercase(df):\n",
    "    df['upper_case'] = df['tweet'].apply(lambda x: len([x for x in x.split() if x.isupper()]))\n",
    "    print(df[['tweet','upper_case']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               tweet  upper_case\n",
      "0   @user when a father is dysfunctional and is s...           0\n",
      "1  @user @user thanks for #lyft credit i can't us...           0\n",
      "2                                bihday your majesty           0\n",
      "3  #model   i love u take with u all the time in ...           0\n",
      "4             factsguide: society now    #motivation           0\n"
     ]
    }
   ],
   "source": [
    "num_uppercase(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               tweet  upper_case\n",
      "0  #studiolife #aislife #requires #passion #dedic...           0\n",
      "1   @user #white #supremacists want everyone to s...           0\n",
      "2  safe ways to heal your #acne!!    #altwaystohe...           0\n",
      "3  is the hp and the cursed child book up for res...           0\n",
      "4    3rd #bihday to my amazing, hilarious #nephew...           0\n"
     ]
    }
   ],
   "source": [
    "num_uppercase(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3. Основна обработка на текст** <a class=\"anchor\" id=\"3\"></a>\n",
    "\n",
    "\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "\n",
    "- Досега се научихме как да извличаме основни характеристики от текстови данни.\n",
    "\n",
    "- Сега преминаваме към извличане на текст и характеристики. \n",
    "\n",
    "- Първата ни стъпка трябва да бъде да изчистим данните, за да получим по-добри характеристики.\n",
    "\n",
    "- Ще постигнем това, като извършим някои от основните стъпки за предварителна обработка на нашите данни за обучение.\n",
    "\n",
    "- И така, нека да се запознаем с това."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.1 CountVectorization** <a class=\"anchor\" id=\"3.1\"></a>\n",
    "\n",
    "\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "\n",
    "- **[CounterVectorization](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)** е библиотека SciKitLearn, която приема всеки текстов документ и връща всяка уникална дума като характеристика с броя на срещанията на тази дума.\n",
    " \n",
    "- Tова може да генерира много функции с някои изключително полезни параметри, които помагат да се избегне това, включително stop_words, n_grams и max_features.\n",
    "\n",
    "- **Stop words** генерира списък с думи, които няма да бъдат включени като функция. Основната употреба на това е „английският“ речник, където ще се отърве от незначителни думи като „is, the, a, it“, като „които могат да се появяват доста често, но имат малко или никакво влияние върху нашата крайна цел.\n",
    "\n",
    "- **ngrams_range** избира как можете да групирате думите заедно. Вместо NLP да връща всяка дума поотделно, можем да получим резултати като „Здравей отново“, ако е равно на 2 или „Ще се видим по-късно“, ако е равно на 3.\n",
    "\n",
    "- **max_features** е колко функции сте избрали да създадете. Ако изберем да е равно на нито една, това означава, че ще получим всички и всички думи като характеристики, но ако го зададем равно на 50, ще получите само 50-те най-често използвани думи.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CountVectorizer' object has no attribute 'get_feature_names'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m vectorizer \u001b[38;5;241m=\u001b[39m CountVectorizer()\n\u001b[0;32m     10\u001b[0m X \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mfit_transform(corpus)\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28mprint\u001b[39m(vectorizer\u001b[38;5;241m.\u001b[39mget_feature_names())\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'CountVectorizer' object has no attribute 'get_feature_names'"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = [\n",
    "          'This is the first document.',\n",
    "          'This document is the second document.',\n",
    "          'And this is the third one.',\n",
    "          'Is this the first document?',\n",
    "         ]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 1 0 0 1 0 1]\n",
      " [0 2 0 1 0 1 1 0 1]\n",
      " [1 0 0 1 1 0 1 1 1]\n",
      " [0 1 1 1 0 0 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CountVectorizer' object has no attribute 'get_feature_names'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m vectorizer2 \u001b[38;5;241m=\u001b[39m CountVectorizer(analyzer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mword\u001b[39m\u001b[38;5;124m'\u001b[39m, ngram_range\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m))\n\u001b[0;32m      2\u001b[0m X2 \u001b[38;5;241m=\u001b[39m vectorizer2\u001b[38;5;241m.\u001b[39mfit_transform(corpus)\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(vectorizer2\u001b[38;5;241m.\u001b[39mget_feature_names())\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'CountVectorizer' object has no attribute 'get_feature_names'"
     ]
    }
   ],
   "source": [
    "vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n",
    "X2 = vectorizer2.fit_transform(corpus)\n",
    "print(vectorizer2.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 1 0 0 1 0 0 0 0 1 0]\n",
      " [0 1 0 1 0 1 0 1 0 0 1 0 0]\n",
      " [1 0 0 1 0 0 0 0 1 1 0 1 0]\n",
      " [0 0 1 0 1 0 1 0 0 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "print(X2.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.2 HashingVectorizer** <a class=\"anchor\" id=\"3.2\"></a>\n",
    "\n",
    "\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "\n",
    "- [Hashing Vectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html) преобразува текста в матрица от събития, използвайки „hashing trick“.\n",
    "\n",
    "\n",
    "- Преобразува колекция от текстови документи в матрица от срещания на токени.\n",
    "\n",
    "- Превръща колекция от текстови документи в scipy.sparse матрица, съдържаща броя на срещанията на токени (или двоична информация за срещанията), евентуално нормализирани като честоти на токени, ако norm=’l1’ или проектирани върху сферата на евклидовата единица, ако norm=’l2’.\n",
    "\n",
    "- Това внедряване на векторизатор на текст използва трика за хеширане, за да намери името на низа на токена, за да включва съпоставяне на целочислен индекс.\n",
    "\n",
    "- Всяка дума е съпоставена с функция и използването на хеш функцията я преобразува в хеш.\n",
    "\n",
    "- Ако думата се появи отново в тялото на текста, тя се преобразува в същата функция, която ни позволява да я броим в същата функция, без да запазваме речник в паметта."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 16)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "corpus = [\n",
    "          'This is the first document.',\n",
    "          'This document is the second document.',\n",
    "          'And this is the third one.',\n",
    "          'Is this the first document?',\n",
    "         ]\n",
    "vectorizer = HashingVectorizer(n_features=2**4)\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.3 Lower Casing(Малки букви)** <a class=\"anchor\" id=\"3.3\"></a>\n",
    "\n",
    "\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "\n",
    "- Друга стъпка на предварителна обработка, която ще направим, е да трансформираме нашите туитове в малки букви.\n",
    "- Това избягва наличието на множество копия на едни и същи думи.\n",
    "- Например, докато изчислявате броя на думите, „Аналитика“ и „аналитика“ ще се приемат като различни думи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_case(df):\n",
    "    df['tweet'] = df['tweet'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "    print(df['tweet'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    @user when a father is dysfunctional and is so...\n",
      "1    @user @user thanks for #lyft credit i can't us...\n",
      "2                                  bihday your majesty\n",
      "3    #model i love u take with u all the time in ur...\n",
      "4                  factsguide: society now #motivation\n",
      "Name: tweet, dtype: object\n"
     ]
    }
   ],
   "source": [
    "lower_case(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    #studiolife #aislife #requires #passion #dedic...\n",
      "1    @user #white #supremacists want everyone to se...\n",
      "2    safe ways to heal your #acne!! #altwaystoheal ...\n",
      "3    is the hp and the cursed child book up for res...\n",
      "4    3rd #bihday to my amazing, hilarious #nephew e...\n",
      "Name: tweet, dtype: object\n"
     ]
    }
   ],
   "source": [
    "lower_case(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.4 Punctuation Removal(Премахване на пунктуацията)** <a class=\"anchor\" id=\"3.4\"></a>\n",
    "\n",
    "\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "\n",
    "- Следващата стъпка е да премахнете препинателните знаци, тъй като не добавя допълнителна информация, докато обработва текстови данни.\n",
    "\n",
    "- Следователно премахването на всички негови екземпляри ще ни помогне да намалим размера на данните за обучение."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def punctuation_removal(df):\n",
    "    df['tweet'] = df['tweet'].str.replace('[^\\w\\s]','')\n",
    "    print(df['tweet'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    @user when a father is dysfunctional and is so...\n",
      "1    @user @user thanks for #lyft credit i can't us...\n",
      "2                                  bihday your majesty\n",
      "3    #model i love u take with u all the time in ur...\n",
      "4                  factsguide: society now #motivation\n",
      "Name: tweet, dtype: object\n"
     ]
    }
   ],
   "source": [
    "punctuation_removal(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    #studiolife #aislife #requires #passion #dedic...\n",
      "1    @user #white #supremacists want everyone to se...\n",
      "2    safe ways to heal your #acne!! #altwaystoheal ...\n",
      "3    is the hp and the cursed child book up for res...\n",
      "4    3rd #bihday to my amazing, hilarious #nephew e...\n",
      "Name: tweet, dtype: object\n"
     ]
    }
   ],
   "source": [
    "punctuation_removal(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Виждаме, че всички препинателни знаци, включително „#“ и „@“, са премахнати от данните за обучението."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.5 Stop Words Removal(Спиране на премахването на думи)**  <a class=\"anchor\" id=\"3.5\"></a>\n",
    "\n",
    "\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "\n",
    "- Както обсъдихме по-рано, спиращите думи (или често срещаните думи) трябва да бъдат премахнати от текстовите данни.\n",
    "- За тази цел можем или сами да създадем списък със стоп думи, или можем да използваме предварително дефинирани библиотеки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_words_removal(df):\n",
    "    df['tweet'] = df['tweet'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "    print(df['tweet'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    @user father dysfunctional selfish drags kids ...\n",
      "1    @user @user thanks #lyft credit can't use caus...\n",
      "2                                       bihday majesty\n",
      "3    #model love u take u time urð±!!! ððð...\n",
      "4                      factsguide: society #motivation\n",
      "Name: tweet, dtype: object\n"
     ]
    }
   ],
   "source": [
    "stop_words_removal(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    #studiolife #aislife #requires #passion #dedic...\n",
      "1    @user #white #supremacists want everyone see n...\n",
      "2    safe ways heal #acne!! #altwaystoheal #healthy...\n",
      "3    hp cursed child book reservations already? yes...\n",
      "4    3rd #bihday amazing, hilarious #nephew eli ahm...\n",
      "Name: tweet, dtype: object\n"
     ]
    }
   ],
   "source": [
    "stop_words_removal(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.6 Frequent Words Removal**  <a class=\"anchor\" id=\"3.6\"></a>\n",
    "\n",
    "\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "\n",
    "- Можем също така да премахваме често срещани думи от нашите текстови данни.\n",
    "\n",
    "- Първо, нека проверим 10-те най-често срещани думи в нашите текстови данни, след което приемем повикване, за да премахнем или запазим."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "@user    17291\n",
       "&amp;     1574\n",
       "day       1454\n",
       "#love     1449\n",
       "happy     1328\n",
       "-         1244\n",
       "u         1116\n",
       "love      1112\n",
       "i'm        992\n",
       "like       920\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq = pd.Series(' '.join(train['tweet']).split()).value_counts()[:10]\n",
    "freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сега ще премахнем тези думи, тъй като тяхното присъствие няма да е от полза при класифицирането на нашите текстови данни."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = list(freq.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequent_words_removal(df):    \n",
    "    df['tweet'] = df['tweet'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n",
    "    print(df['tweet'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_words_removal(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_words_removal(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.7 Rare Words Removal**  <a class=\"anchor\" id=\"3.7\"></a>\n",
    "\n",
    "\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "\n",
    "- Сега ще премахнем рядко срещаните думи от текста.\n",
    "- Тъй като са толкова редки, връзката между тях и други думи е доминирана от шум.\n",
    "- Можем да заменим редки думи с по-обща форма и тогава това ще има по-висок брой."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = pd.Series(' '.join(train['tweet']).split()).value_counts()[-10:]\n",
    "freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = list(freq.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rare_words_removal(df):\n",
    "    df['tweet'] = df['tweet'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n",
    "    print(df['tweet'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rare_words_removal(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rare_words_removal(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Всички тези стъпки на предварителна обработка са от съществено значение и ни помагат да намалим бъркотията в нашия речник, така че създадените в крайна сметка характеристики да са по-ефективни."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.8 Spelling Correction**  <a class=\"anchor\" id=\"3.8\"></a>\n",
    "\n",
    "\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "\n",
    "\n",
    "- Сега туитовете могат да бъдат пълни с множество правописни грешки. Нашата задача е да поправим тези правописни грешки.\n",
    "\n",
    "- В този контекст корекцията на правописа е полезна стъпка за предварителна обработка, защото това също ще ни помогне да намалим множеството копия на думи. Например „Analytics“ и „analytcs“ ще се третират като различни думи, дори ако се използват в същия смисъл.\n",
    "\n",
    "- За да изпълним горната задача, ще използваме библиотеката на текстови блокове, както следва-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spell_correction(df):\n",
    "    return df['tweet'][:5].apply(lambda x: str(TextBlob(x).correct()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spell_correction(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spell_correction(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.9 Tokenization** <a class=\"anchor\" id=\"3.9\"></a>\n",
    "\n",
    "\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "\n",
    "- [Tokenization](https://www.geeksforgeeks.org/nlp-how-tokenizing-text-sentence-words-works/) се отнася до разделяне на текста на поредица от думи или изречения.\n",
    "\n",
    "- В нашия пример използвахме библиотеката с текстови блокове, за да трансформираме първо нашите туитове в петно ​​и след това да ги преобразуваме в поредица от думи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens(df):\n",
    "    return TextBlob(df['tweet'][1]).words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.10 Stemming** <a class=\"anchor\" id=\"3.10\"></a>\n",
    "\n",
    "\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "\n",
    "- [Stemming](https://www.geeksforgeeks.org/introduction-to-stemming/) се отнася до премахването на достатъчно, като \"ing\", \"ly\", \"s\" и т.н. чрез прост подход, базиран на правила.\n",
    "\n",
    "- И така, произходът взема дума и я препраща обратно към нейната основа или коренна форма. **Stems**, **Stemming**, **Stemmed** и **Stemtization** се базират на една дума **stemming**.\n",
    "\n",
    "- За целта ще използваме *PorterStemmer* от NLTK библиотеката."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "st = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(df):\n",
    "    return df['tweet'][:5].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemming(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemming(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can see that *dysfunctional* has been transformed into *dysfunct*, among other changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.11 Лематизацията** <a class=\"anchor\" id=\"3.11\"></a>\n",
    "\n",
    "\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "\n",
    "- [Лематизацията](https://www.geeksforgeeks.org/python-lemmatization-with-nltk/) е процес на преобразуване на дума в нейната основна форма. Разликата между корена и лематизацията е, че лематизацията взема предвид контекста и преобразува думата в нейната смислена основна форма, докато коренът просто премахва последните няколко знака, което често води до неправилни значения и правописни грешки.\n",
    "\n",
    "- Лематизацията е по-ефективен вариант от произтичането, защото преобразува думата в нейната коренна дума, а не просто премахва суфициите.\n",
    "\n",
    "- Лематизацията използва речника и прави морфологичен анализ, за ​​да получи корена на думата. Затова обикновено предпочитаме да използваме лематизацията пред stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(df):\n",
    "    df['tweet'] = df['tweet'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
    "    print(df['tweet'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatization(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatization(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **4. Advanced Text Processing** <a class=\"anchor\" id=\"4\"></a>\n",
    "\n",
    "\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "\n",
    "- Until now, we have covered all the basic pre-processing steps which are helpful in order to clean our data.\n",
    "\n",
    "- Now,we will move on and focus on advanced text-processing or NLP techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.1 N-grams** <a class=\"anchor\" id=\"4.1\"></a>\n",
    "\n",
    "\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "\n",
    "- [N-grams](https://kavita-ganesan.com/what-are-n-grams/#.Xo3jccgzbIU) са комбинация от няколко думи, използвани заедно. Ngrams с N=1 се наричат ​​**униграми**. По същия начин, **биграми (N=2)**, **триграми (N=3)** и т.н.\n",
    "\n",
    "- **Униграмите** обикновено не съдържат толкова много информация в сравнение с **биграмите** и **триграмите**. Основният принцип зад n-грамите е, че те улавят езиковата структура, като например коя буква или дума е вероятно да последва дадената.\n",
    "\n",
    "- Колкото по-дълъг е n-gram (колкото по-високо е n), с толкова повече контекст трябва да работите. Оптималната дължина наистина зависи от приложението – ако вашите n-грамове са твърде къси, може да не успеете да уловите важни разлики. От друга страна, ако са твърде дълги, може да не успеете да уловите „общите знания“ и да се придържате само към конкретни случаи.\n",
    "\n",
    "- Сега ще извлечем биграми от нашите туитове с помощта на функцията ngrams на библиотеката textblob."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combination_of_words(df):\n",
    "    return (TextBlob(df['tweet'][0]).ngrams(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combination_of_words(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combination_of_words(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **4.2 Term Frequency**  <a class=\"anchor\" id=\"4.2\"></a>\n",
    "\n",
    "\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "\n",
    "- Честотата на термина е просто съотношението на броя на думата, присъстваща в изречението, към дължината на изречението.\n",
    "\n",
    "- Следователно можем да обобщим термина честота като:\n",
    "\n",
    "- TF = (Брой пъти, когато термин T се появява в конкретния ред) / (брой термини в този ред)\n",
    "\n",
    "- Ще създадем таблица с термини и честоти на туит, както следва-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def term_frequency(df):\n",
    "    tf1 = (df['tweet'][1:2]).apply(lambda x: pd.value_counts(x.split(\" \"))).sum(axis = 0).reset_index()\n",
    "    tf1.columns = ['words','tf']\n",
    "    return tf1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_frequency(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_frequency(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.3 Inverse Document Frequency (IDF)** <a class=\"anchor\" id=\"4.3\"></a>\n",
    "\n",
    "\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "\n",
    "- Интуитивността зад обратната честота на документа (IDF) е, че една дума не ни е много полезна, ако се появява във всички документи.\n",
    "\n",
    "- Следователно IDF на всяка дума е логаритъм на съотношението на общия брой редове към броя редове, в които тази дума присъства.\n",
    "\n",
    "- IDF може да се изчисли, както следва -\n",
    "\n",
    "   **IDF = log(N/n)**,\n",
    "   \n",
    " където N е общият брой редове, а n е броят редове, в които думата присъства.\n",
    "\n",
    "- Сега ще изчислим IDF за същите туитове, за които изчислихме честотата на термина."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf1 = (train['tweet'][1:2]).apply(lambda x: pd.value_counts(x.split(\" \"))).sum(axis = 0).reset_index()\n",
    "tf1.columns = ['words','tf']\n",
    "tf1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf2 = (test['tweet'][1:2]).apply(lambda x: pd.value_counts(x.split(\" \"))).sum(axis = 0).reset_index()\n",
    "tf2.columns = ['words','tf']\n",
    "tf2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Колкото по-голяма е стойността на IDF, толкова по-уникална е думата."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.4 Term Frequency – Inverse Document Frequency (TF-IDF)** <a class=\"anchor\" id=\"4.4\"></a>\n",
    "\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "\n",
    "- **TF-IDF** е умножението на TF и ​​IDF, което изчислихме отново по-долу за удобство."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf1 = (train['tweet'][1:2]).apply(lambda x: pd.value_counts(x.split(\" \"))).sum(axis = 0).reset_index()\n",
    "tf1.columns = ['words','tf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,word in enumerate(tf1['words']):\n",
    "    tf1.loc[i, 'idf'] = np.log(train.shape[0]/(len(train[train['tweet'].str.contains(word)])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf1['tfidf'] = tf1['tf'] * tf1['idf']\n",
    "tf1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Можем да видим, че TF-IDF е наказал думи като „не“, „не мога“ и „използвайте“, защото те са често срещани думи. Въпреки това, той даде голяма тежест на „разочарован“, тъй като това ще бъде много полезно при определяне на настроението на туита.\n",
    "\n",
    "- Не е нужно да изчисляваме TF и ​​IDF всеки път предварително и след това да ги умножаваме, за да получим TF-IDF. Вместо това sklearn има отделна функция за директно получаване:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(max_features=1000, lowercase=True, analyzer='word',\n",
    " stop_words= 'english',ngram_range=(1,1))\n",
    "train_vect = tfidf.fit_transform(train['tweet'])\n",
    "train_vect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.5 Bag of Words** <a class=\"anchor\" id=\"4.5\"></a>\n",
    "\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "\n",
    "- [Bag of Words (BoW)](https://machinelearningmastery.com/gentle-introduction-bag-words-model/) се отнася до представянето на текст, което описва присъствието на думи в текстовите данни. Интуитивността зад това е, че две подобни текстови полета ще съдържат подобен вид думи и следователно ще имат подобен пакет от думи. Освен това, че само от текста можем да научим нещо за значението на документа.\n",
    "\n",
    "- За изпълнение sklearn предоставя отделна функция за него, както е показано по-долу:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "bow = CountVectorizer(max_features=1000, lowercase=True, ngram_range=(1,1),analyzer = \"word\")\n",
    "train_bow = bow.fit_transform(train['tweet'])\n",
    "train_bow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.6 Sentiment Analysis**  <a class=\"anchor\" id=\"4.6\"></a>\n",
    "\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "\n",
    "- Сега стигаме до нашия проблем, който беше да открием настроението на туита. Така че, преди да приложите каквито и да е ML/DL модели (които могат да имат отделна функция, откриваща настроението с помощта на библиотеката на текстови блокове).\n",
    "\n",
    "- Ще проверим настроението на първите няколко туита, както следва -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polarity_subjectivity(df):\n",
    "    return df['tweet'][:5].apply(lambda x: TextBlob(x).sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polarity_subjectivity(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polarity_subjectivity(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можем да видим, че връща tuple, представляващ полярността и субективността на всеки туит. Тук извличаме само полярността, тъй като тя показва настроението, тъй като стойността, по-близка до 1, означава положителна нагласа, а стойностите, по-близки до -1, означават отрицателна нагласа. Това може да работи и като функция за изграждане на модел за машинно обучение."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analysis(df):\n",
    "    df['sentiment'] = df['tweet'].apply(lambda x: TextBlob(x).sentiment[0] )\n",
    "    return df[['tweet','sentiment']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_analysis(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_analysis(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **5. References** <a class=\"anchor\" id=\"5\"></a>\n",
    "\n",
    "\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "\n",
    "Този бележник е базиран на отлична статия от Shubham Jain -\n",
    "\n",
    "- [Ultimate Guide to deal with Text Data](https://www.analyticsvidhya.com/blog/2018/02/the-different-methods-deal-text-data-predictive-python/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обсъдихме няколко общи начина за работа с текстови данни в този бележник. Тези методи ще ни дадат основно разбиране за това как да се справяме с текстови данни при прогнозно моделиране."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Надявам се да намерите тази тетрадка за полезна и приятна. Вашите коментари и отзиви са добре дошли.\n",
    "\n",
    "Благодаря ви"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Go to Top](#0)\t"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
