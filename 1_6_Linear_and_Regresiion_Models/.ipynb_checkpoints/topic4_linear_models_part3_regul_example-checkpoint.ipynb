{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"../../img/ods_stickers.jpg\" />\n",
    "    \n",
    "## [mlcourse.ai](https://mlcourse.ai) – Отворен курс за машинно обучение\n",
    "\n",
    "Автор: [Юрий Кашницки](https://yorko.github.io). Превод и редакция от [Christina Butsko](https://www.linkedin.com/in/christinabutsko/), [Nerses Bagiyan](https://www.linkedin.com/in/nersesbagiyan/), [Yulia Klimushina] (https://www.linkedin.com/in/yuliya-klimushina-7168a9139) и [Yuanyuan Pao](https://www.linkedin.com/in/yuanyuanpao/). Този материал е предмет на правилата и условията на лиценза [Creative Commons CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/). Безплатното използване е разрешено за всякакви нетърговски цели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Тема 4. Линейна класификация и регресия\n",
    "## <center> Част 3. Илюстративен пример за регулиране на логистична регресия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В първата статия демонстрирахме как полиномните характеристики позволяват на линейните модели да изграждат нелинейни разделителни повърхности. Нека сега да покажем това визуално.\n",
    "\n",
    "Нека видим как регулирането влияе върху качеството на класификацията на набор от данни при тестване на микрочипове от курса на Андрю Нг за машинно обучение. Ще използваме логистична регресия с полиномиални характеристики и ще променяме параметъра за регуляризация $C$. Първо, ще видим как регулирането влияе на разделителната граница на класификатора и интуитивно ще разпознаем недостатъчното и прекомерното приспособяване. След това ще изберем параметъра за регулиране да бъде числено близо до оптималната стойност чрез (`кръстосано валидиране`) и (`GridSearch`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we don't like warnings\n",
    "# you can comment the following 2 lines if you'd like to\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.model_selection import (GridSearchCV, StratifiedKFold,\n",
    "                                     cross_val_score)\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нека заредим данните с помощта на `read_csv` от библиотеката `pandas`. В този набор от данни за 118 микрочипове (обекти) има резултати от два теста за контрол на качеството (две цифрови променливи) и информация дали микрочипът е пуснат в производство. Променливите вече са центрирани, което означава, че средните стойности на колоните са извадени. По този начин \"средният\" микрочип съответства на нулева стойност в резултатите от теста.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading data\n",
    "data = pd.read_csv(\n",
    "    \"../../data/microchip_tests.txt\", header=None, names=(\"test1\", \"test2\", \"released\")\n",
    ")\n",
    "# getting some info about dataframe\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нека инспектираме първите и последните 5 реда."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сега трябва да запазим набора за обучение и етикетите на целевия клас в отделни масиви NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:, :2].values\n",
    "y = data.iloc[:, 2].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Като междинна стъпка можем да начертаем данните. Оранжевите точки съответстват на дефектни чипове, сините - на нормални."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[y == 1, 0], X[y == 1, 1], c=\"blue\", label=\"Released\")\n",
    "plt.scatter(X[y == 0, 0], X[y == 0, 1], c=\"orange\", label=\"Faulty\")\n",
    "plt.xlabel(\"Test 1\")\n",
    "plt.ylabel(\"Test 2\")\n",
    "plt.title(\"2 tests of microchips. Logit with C=1\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нека дефинираме функция за показване на разделителната крива на класификатора."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_boundary(clf, X, y, grid_step=0.01, poly_featurizer=None):\n",
    "    x_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1\n",
    "    y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1\n",
    "    xx, yy = np.meshgrid(\n",
    "        np.arange(x_min, x_max, grid_step), np.arange(y_min, y_max, grid_step)\n",
    "    )\n",
    "\n",
    "    # to every point from [x_min, m_max]x[y_min, y_max]\n",
    "    # we put in correspondence its own color\n",
    "    Z = clf.predict(poly_featurizer.transform(np.c_[xx.ravel(), yy.ravel()]))\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.contour(xx, yy, Z, cmap=plt.cm.Paired)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ние дефинираме следните характеристики на полином от степен $d$ за две променливи $x_1$ и $x_2$:\n",
    "\n",
    "$$\\large \\{x_1^d, x_1^{d-1}x_2, \\ldots x_2^d\\} = \\{x_1^ix_2^j\\}_{i+j=d, i,j \\in \\ mathbb{N}}$$\n",
    "\n",
    "Например, за $d=3$, това ще бъдат следните характеристики:\n",
    "\n",
    "$$\\large 1, x_1, x_2, x_1^2, x_1x_2, x_2^2, x_1^3, x_1^2x_2, x_1x_2^2, x_2^3$$\n",
    "\n",
    "Начертаването на Питагоров триъгълник ще покаже колко от тези характеристики ще има за $d=4,5...$ и т.н.\n",
    "Броят на такива характеристики е експоненциално голям и може да бъде скъпо да се изградят полиномни характеристики с голяма степен (напр. $d=10$) за 100 променливи. По-важното е, че не е необходимо.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ще използваме изпълнението на логистична регресия на `sklearn`. И така, създаваме обект, който ще добави полиномни характеристики до степен 7 към матрицата $X$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures(degree=7)\n",
    "X_poly = poly.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_poly.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нека обучим логистична регресия с параметър за регулация $C = 10^{-2}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = 1e-2\n",
    "logit = LogisticRegression(C=C, random_state=17)\n",
    "logit.fit(X_poly, y)\n",
    "\n",
    "plot_boundary(logit, X, y, grid_step=0.01, poly_featurizer=poly)\n",
    "\n",
    "plt.scatter(X[y == 1, 0], X[y == 1, 1], c=\"blue\", label=\"Released\")\n",
    "plt.scatter(X[y == 0, 0], X[y == 0, 1], c=\"orange\", label=\"Faulty\")\n",
    "plt.xlabel(\"Test 1\")\n",
    "plt.ylabel(\"Test 2\")\n",
    "plt.title(\"2 tests of microchips. Logit with C=%s\" % C)\n",
    "plt.legend()\n",
    "\n",
    "print(\"Accuracy on training set:\", round(logit.score(X_poly, y), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сега можем да опитаме да увеличим $C$ до 1. Правейки това, ние отслабваме регуляризацията и решението вече може да има по-големи стойности (в абсолютна стойност) на теглата на модела от преди. Сега точността на класификатора на набора за обучение се подобрява до 0,831."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = 1\n",
    "logit = LogisticRegression(C=C, random_state=17)\n",
    "logit.fit(X_poly, y)\n",
    "\n",
    "plot_boundary(logit, X, y, grid_step=0.005, poly_featurizer=poly)\n",
    "\n",
    "plt.scatter(X[y == 1, 0], X[y == 1, 1], c=\"blue\", label=\"Released\")\n",
    "plt.scatter(X[y == 0, 0], X[y == 0, 1], c=\"orange\", label=\"Faulty\")\n",
    "plt.xlabel(\"Test 1\")\n",
    "plt.ylabel(\"Test 2\")\n",
    "plt.title(\"2 tests of microchips. Logit with C=%s\" % C)\n",
    "plt.legend()\n",
    "\n",
    "print(\"Accuracy on training set:\", round(logit.score(X_poly, y), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тогава защо не увеличим $C$ още повече - до 10 000? Сега регулирането очевидно не е достатъчно силно и виждаме пренастройване. Обърнете внимание, че при $C$=1 и \"плавна\" граница делът на правилните отговори в набора за обучение не е много по-нисък от тук. Но човек може лесно да си представи как нашият втори модел ще работи много по-добре върху нови данни.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = 1e4\n",
    "logit = LogisticRegression(C=C, random_state=17)\n",
    "logit.fit(X_poly, y)\n",
    "\n",
    "plot_boundary(logit, X, y, grid_step=0.005, poly_featurizer=poly)\n",
    "\n",
    "plt.scatter(X[y == 1, 0], X[y == 1, 1], c=\"blue\", label=\"Released\")\n",
    "plt.scatter(X[y == 0, 0], X[y == 0, 1], c=\"orange\", label=\"Faulty\")\n",
    "plt.xlabel(\"Test 1\")\n",
    "plt.ylabel(\"Test 2\")\n",
    "plt.title(\"2 tests of microchips. Logit with C=%s\" % C)\n",
    "plt.legend()\n",
    "\n",
    "print(\"Accuracy on training set:\", round(logit.score(X_poly, y), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "За да обсъдим резултатите, нека пренапишем функцията, която е оптимизирана в логистична регресия с формата:\n",
    "\n",
    "$$\\large J(X,y,w) = \\mathcal{L} + \\frac{1}{C}||w||^2,$$\n",
    "\n",
    "където\n",
    "\n",
    "- $\\mathcal{L}$ е функцията на логистичните загуби, сумирана за целия набор от данни\n",
    "- $C$ е коефициентът на обратна регулация (същият $C$ от изпълнението на `sklearn` на `LogisticRegression`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Междинни суми**:\n",
    "- колкото по-голям е параметърът $C$, толкова по-сложни са връзките в данните, които моделът може да възстанови (интуитивно $C$ съответства на \"сложността\" на модела - капацитет на модела)\n",
    "- ако регулирането е твърде силно, т.е. стойностите на $C$ са малки, решението на проблема с минимизирането на функцията на логистичните загуби може да бъде това, при което много от теглата са твърде малки или нулирани. Моделът също не е достатъчно \"наказан\" за грешки (т.е. във функцията $J$ сумата от квадратите на теглата \"претегля\" и грешката $\\mathcal{L}$ може да бъде относително голяма). В този случай моделът няма да пасне, както видяхме в първия случай.\n",
    "- напротив, ако регуляризацията е твърде слаба, т.е. стойностите на $C$ са големи, вектор $w$ с компоненти с висока абсолютна стойност може да се превърне в решение на проблема с оптимизацията. В този случай $\\mathcal{L}$ има по-голям принос към оптимизирания функционал $J$. Свободно казано, моделът се „страхува“ твърде много да бъде сбъркан с обектите от обучителния набор и следователно ще прекалява, както видяхме в третия случай.\n",
    "- логистичната регресия няма да \"разбере\" (или \"научи\") каква стойност на $C$ да избере, както се случва с теглата $w$. Тоест, не може да се определи чрез решаване на проблема за оптимизация в логистична регресия. Виждали сме подобна ситуация и преди - дървото на решенията не може да \"научи\" каква граница на дълбочина да избере по време на процеса на обучение. Следователно $C$ е хиперпараметърът на модела, който се настройва при кръстосано валидиране; както и max_depth в дърво."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Regularization parameter tuning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Използвайки този пример, нека идентифицираме оптималната стойност на параметъра за регулиране $C$. Това може да се направи с помощта на `LogisticRegressionCV` - търсене в мрежа на параметри, последвано от кръстосано валидиране. Този клас е проектиран специално за логистична регресия (ефективни алгоритми с добре известни параметри за търсене). За произволен модел използвайте `GridSearchCV`, `RandomizedSearchCV` или специални алгоритми за оптимизиране на хиперпараметри, като този, внедрен в `hyperopt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=17)\n",
    "\n",
    "c_values = np.logspace(-2, 3, 500)\n",
    "\n",
    "logit_searcher = LogisticRegressionCV(Cs=c_values, cv=skf, verbose=1, n_jobs=-1)\n",
    "logit_searcher.fit(X_poly, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_searcher.C_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "За да видим как качеството на модела (процент на правилните отговори на наборите за обучение и валидиране) варира в зависимост от хиперпараметъра $C$, можем да начертаем графиката."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(c_values, np.mean(logit_searcher.scores_[1], axis=0))\n",
    "plt.xlabel(\"C\")\n",
    "plt.ylabel(\"Mean CV-accuracy\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Накрая изберете областта с \"най-добрите\" стойности на $C$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(c_values, np.mean(logit_searcher.scores_[1], axis=0))\n",
    "plt.xlabel(\"C\")\n",
    "plt.ylabel(\"Mean CV-accuracy\")\n",
    "plt.xlim((0, 10));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Спомнете си, че тези криви се наричат ​​валидационни криви. Преди ги създавахме ръчно, но sklearn има специални методи за конструирането им, които ще използваме занапред."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Полезни ресурси\n",
    "- Основен курс [сайт](https://mlcourse.ai), [репо за курс](https://github.com/Yorko/mlcourse.ai) и YouTube [канал](https://www.youtube. com/watch?v=QKTuw4PNOsU&list=PLVlY_7IJCMJeRfZ68eVfEcu-UcN9BbwiX)\n",
    "- Средна [\"история\"](https://medium.com/open-machine-learning-course/open-machine-learning-course-topic-4-linear-classification-and-regression-44a41b9b5220) въз основа на този бележник\n",
    "- Материали за курса като [Набор от данни на Kaggle](https://www.kaggle.com/kashnitsky/mlcourse)\n",
    "- Ако четете руски: [статия](https://habrahabr.ru/company/ods/blog/323890/) на Habrahabr със ~ същия материал. И [лекция](https://youtu.be/oTXGQ-_oqvI) в YouTube\n",
    "- Хубав и кратък преглед на линейните модели е даден в книгата [“Deep Learning”](http://www.deeplearningbook.org) (I. Goodfellow, Y. Bengio и A. Courville).\n",
    "- Линейните модели са разгледани практически във всяка книга за ML. Препоръчваме „Разпознаване на модели и машинно обучение“ (C. Bishop) и „Machine Learning: A Probabilistic Perspective“ (K. Murphy).\n",
    "- Ако предпочитате задълбочен преглед на линейния модел от гледна точка на статистик, тогава вижте „Елементите на статистическото обучение“ (T. Hastie, R. Tibshirani и J. Friedman).\n",
    "- Книгата „Машинно обучение в действие“ (P. Harrington) ще ви преведе през имплементации на класически ML алгоритми в чист Python.\n",
    "- [Scikit-learn](http://scikit-learn.org/stable/documentation.html) библиотека. Тези момчета работят усилено върху писането на наистина ясна документация.\n",
    "- Scipy 2017 [урок за scikit-learn](https://github.com/amueller/scipy-2017-sklearn) от Алекс Грамфорт и Андреас Мюлер.\n",
    "- Още един [ML курс](https://github.com/diefimov/MTH594_MachineLearning) с много добри материали.\n",
    "- [Имплементации](https://github.com/rushter/MLAlgorithms) на много ML алгоритми. Търсете линейна регресия и логистична регресия.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
