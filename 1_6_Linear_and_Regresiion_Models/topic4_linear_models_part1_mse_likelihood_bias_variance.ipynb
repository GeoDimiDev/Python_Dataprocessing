{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"../../img/ods_stickers.jpg\" />\n",
    "    \n",
    "## [mlcourse.ai](https://mlcourse.ai) – Отворен курс за машинно обучение\n",
    "    \n",
    "Автор: [Павел Нестеров](http://pavelnesterov.info/). Преведено и редактирано от [Кристина Буцко](https://www.linkedin.com/in/christinabutsko/), [Юрий Кашницки](https://yorko.github.io), [Нерсес Багиян](https:// www.linkedin.com/in/nersesbagiyan/), [Юлия Климушина](https://www.linkedin.com/in/yuliya-klimushina-7168a9139) и [Юанюан Пао](https://www.linkedin. com/in/yuanyuanpao/). Този материал е предмет на правилата и условията на лиценза [Creative Commons CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/). Безплатното използване е разрешено за всякакви нетърговски цели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# <center>Тема 4. Линейна класификация и регресия\n",
    "## <center> Част 1. Обикновени най-малки квадрати\n",
    "\n",
    "\n",
    "## Описание на статията\n",
    "1. [Въведение](#1.-Въведение)\n",
    "2. [Оценка на максималната вероятност] (#2.-Оценка на максималната-вероятност)\n",
    "3. [Разлагане на отклонение-вариация](#3.-Разлагане на отклонение-вариация)\n",
    "4. [Регуларизация на линейната регресия](#4.-Регуларизация-на-линейна-регресия)\n",
    "5. [Демо задание](#5.-Демо-задаване)\n",
    "6. [Полезни ресурси](#6.-Полезни-ресурси)\n",
    "\n",
    "\n",
    "## 1. Въведение\n",
    "Ще започнем да изучаваме линейни модели с линейна регресия. На първо място, трябва да посочите модел, който свързва зависимата променлива $y$ с обяснителните фактори (или характеристики); за линейни модели функцията на зависимост ще приеме следната форма: $\\large y = w_0 + \\sum_{i=1}^m w_i x_i$, където $m$ е броят на характеристиките. Ако добавим фиктивно измерение $x_0 = 1$ (наречено _bias_ или _intercept_ term) за всяко наблюдение, тогава линейната форма може да бъде пренаписана по малко по-компактен начин чрез изтегляне на абсолютния член $w_0$ в сумата: $\\large y = \\sum_{i=0}^m w_i x_i = \\textbf{w}^\\text{T} \\textbf{x}$. Ако имаме матрица от $n$ наблюдения, където редовете са наблюдения от набор от данни, трябва да добавим една колона от единици вляво. Ние дефинираме модела, както следва:\n",
    "$$\\large \\textbf y = \\textbf X \\textbf w + \\epsilon,$$\n",
    "\n",
    "където\n",
    "\n",
    "- $\\textbf w \\in \\mathbb{R}^{m+1}$ – е $(m+1) \\times 1$ вектор-колона на параметрите на модела (в машинното обучение тези параметри често се наричат като *тегла*);\n",
    "- $\\textbf X \\in \\mathbb{R}^{n \\times (m+1)}$ – е $n \\times (m+1)$ матрица от наблюдения и техните характеристики (включително фиктивната колона на отляво) с пълна колона [ранг](https://en.wikipedia.org/wiki/Rank_(linear_algebra)): $\\text{rank}\\left(\\textbf X\\right) = m + 1 $;\n",
    "- $\\epsilon \\in \\mathbb{R}^n$ – е $n \\times 1$ случаен вектор-колона, наричан *грешка* или *шум*;\n",
    "- $\\textbf y \\in \\mathbb{R}^n$ – е $n \\times 1$ вектор-колона - зависимата (или *целева*) променлива\n",
    "Можем също да изпишем този израз за всяко наблюдение\n",
    "\n",
    "$$\\large y_i = \\sum_{j=0}^m w_j X_{ij} + \\epsilon_i$$\n",
    "\n",
    "Ще приложи следните ограничения към набора от случайни грешки $\\epsilon_i$ (вижте теоремата [Gauss-Markov](https://en.wikipedia.org/wiki/Gauss%E2%80%93Markov_theorem) за по-дълбока статистическа мотивация):\n",
    "\n",
    "- очакването на всички случайни грешки е нула: $\\forall i: \\mathbb{E}\\left[\\epsilon_i\\right] = 0 $;\n",
    "- всички случайни грешки имат еднаква крайна вариация, това свойство се нарича <a href=\"https://en.wikipedia.org/wiki/Homoscedasticity\">homoscedasticity</a>: $\\forall i: \\text{Var} \\left(\\epsilon_i\\right) = \\sigma^2 < \\infty $;\n",
    "- случайните грешки не са корелирани: $\\forall i \\neq j: \\text{Cov}\\left(\\epsilon_i, \\epsilon_j\\right) = 0 $.\n",
    "\n",
    "[Оценител](https://en.wikipedia.org/wiki/Estimator) $\\widehat{w}_i $ на тегла $w_i $ се нарича *линеен*, ако\n",
    "\n",
    "$$\\large \\widehat{w}_i = \\omega_{1i}y_1 + \\omega_{2i}y_2 + \\cdots + \\omega_{ni}y_n,$$\n",
    "\n",
    "където $\\forall\\ k\\ $, $\\omega_{ki} $\n",
    "зависят само от извадките от $\\textbf X$, но не и от $\\textbf w$ - ние не знаем истинските коефициенти $w_i$ (затова се опитваме да ги оценим с дадените данни). Тъй като решението за намиране на оптималните тегла е линейна оценка, моделът се нарича *линейна регресия*.\n",
    "\n",
    "**Отказ от отговорност:** на този етап е време да кажем, че не планираме да вместим цял курс по статистика в една статия, затова се опитваме да предоставим общата идея за математиката зад OLS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нека въведем още едно определение. Оценител $\\widehat{w}_i $ се нарича *безпристрастен*, когато неговото очакване е равно на реалната, но неизвестна стойност на прогнозния параметър:\n",
    "\n",
    "$$\\Large \\mathbb{E}\\left[\\widehat{w}_i\\right] = w_i$$\n",
    "\n",
    "Един от начините за изчисляване на тези тегла е с обикновения метод на най-малките квадрати (OLS), който минимизира средната квадратична грешка между действителната стойност на зависимата променлива и предвидената стойност, дадена от модела:\n",
    "\n",
    "$$\\Large \\begin{array}{rcl}\\mathcal{L}\\left(\\textbf X, \\textbf{y}, \\textbf{w} \\right) &=& \\frac{1}{2n} \\ sum_{i=1}^n \\left(y_i - \\textbf{w}^\\text{T} \\textbf{x}_i\\right)^2 \\\\\n",
    "&=& \\frac{1}{2n} \\left\\| \\textbf{y} - \\textbf X \\textbf{w} \\right\\|_2^2 \\\\\n",
    "&=& \\frac{1}{2n} \\left(\\textbf{y} - \\textbf X \\textbf{w}\\right)^\\text{T} \\left(\\textbf{y} - \\textbf X \\ textbf{w}\\right)\n",
    "\\end{array}$$\n",
    "\n",
    "За да разрешим този проблем с оптимизацията, трябва да изчислим производни по отношение на параметрите на модела. Поставяме ги на нула и решаваме полученото уравнение за $\\textbf w$ (диференцирането на матрицата може да изглежда трудно; опитайте да го направите по отношение на суми, за да сте сигурни в отговора).\n",
    "\n",
    "<br>\n",
    "<details>\n",
    "<summary>Малък CheatSheet за матрични производни (щракнете върху триъгълника, за да разширите)</summary>\n",
    "<p>\n",
    "$$\\large \\begin{array}{rcl}\n",
    "\\frac{\\partial}{\\partial \\textbf{X}} \\textbf{X}^{\\text{T}} \\textbf{A} &=& \\textbf{A} \\\\\n",
    "\\frac{\\partial}{\\partial \\textbf{X}} \\textbf{X}^{\\text{T}} \\textbf{A} \\textbf{X} &=& \\left(\\textbf{A} + \\textbf{A}^{\\text{T}}\\right)\\textbf{X} \\\\\n",
    "\\frac{\\partial}{\\partial \\textbf{A}} \\textbf{X}^{\\text{T}} \\textbf{A} \\textbf{y} &=& \\textbf{X}^{\\text {T}} \\textbf{y}\\\\\n",
    "\\frac{\\partial}{\\partial \\textbf{X}} \\textbf{A}^{-1} &=& -\\textbf{A}^{-1} \\frac{\\partial \\textbf{A}} {\\partial \\textbf{X}} \\textbf{A}^{-1}\n",
    "\\end{array}$$\n",
    "</p>\n",
    "</details>\n",
    "\n",
    "Това, което получаваме е:\n",
    "\n",
    "$$\\Large \\begin{array}{rcl} \\frac{\\partial \\mathcal{L}}{\\partial \\textbf{w}} &=& \\frac{\\partial}{\\partial \\textbf{w}} \\frac{1}{2n} \\left( \\textbf{y}^{\\text{T}} \\textbf{y} -2\\textbf{y}^{\\text{T}} \\textbf{X} \\ textbf{w} + \\textbf{w}^{\\text{T}} \\textbf{X}^{\\text{T}} \\textbf{X} \\textbf{w}\\right) \\\\\n",
    "&=& \\frac{1}{2n} \\left(-2 \\textbf{X}^{\\text{T}} \\textbf{y} + 2\\textbf{X}^{\\text{T}} \\ textbf{X} \\textbf{w}\\right)\n",
    "\\end{array}$$\n",
    "\n",
    "$$\\Large \\begin{array}{rcl} \\frac{\\partial \\mathcal{L}}{\\partial \\textbf{w}} = 0 &\\Leftrightarrow& \\frac{1}{2n} \\left(-2 \\textbf{X}^{\\text{T}} \\textbf{y} + 2\\textbf{X}^{\\text{T}} \\textbf{X} \\textbf{w}\\right) = 0 \\\\\n",
    "&\\Leftrightarrow& -\\textbf{X}^{\\text{T}} \\textbf{y} + \\textbf{X}^{\\text{T}} \\textbf{X} \\textbf{w} = 0 \\\\\n",
    "&\\Leftrightarrow& \\textbf{X}^{\\text{T}} \\textbf{X} \\textbf{w} = \\textbf{X}^{\\text{T}} \\textbf{y} \\\\\n",
    "&\\Leftrightarrow& \\textbf{w} = \\left(\\textbf{X}^{\\text{T}} \\textbf{X}\\right)^{-1} \\textbf{X}^{\\text{T} } \\textbf{y}\n",
    "\\end{array}$$\n",
    "\n",
    "Като имаме предвид всички дефиниции и условия, описани по-горе, можем да кажем, че въз основа на <a href=\"https://en.wikipedia.org/wiki/Gauss–Markov_theorem\">теоремата на Гаус–Марков</a>, OLS оценителите на параметрите на модела са оптимални сред всички линейни и безпристрастни оценители, т.е. дават най-ниската дисперсия."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Оценка на максималната вероятност\n",
    "\n",
    "Човек може да попита защо избираме да минимизираме средната квадратична грешка вместо нещо друго? В крайна сметка може да се минимизира средната абсолютна стойност на остатъка. Единственото нещо, което ще се случи, ако променим минимизираната стойност, е, че ще надхвърлим условията на теоремата на Гаус-Марков и следователно нашите оценки ще престанат да бъдат оптимални спрямо линейните и безпристрастни.\n",
    "\n",
    "Преди да продължим, нека се отклоним, за да илюстрираме оценката на максималната вероятност с прост пример.\n",
    "\n",
    "Вероятно много хора си спомнят формулата на етилов алкохол, затова реших да направя експеримент, за да установя дали хората си спомнят по-проста формула на метанола: $CH_3OH$. Анкетирахме 400 души, за да установим, че само 117 души си спомнят формулата. Тогава е разумно да се предположи, че вероятността следващият респондент да знае формулата на метиловия алкохол е $\\frac{117}{400} \\approx 29\\%$. Нека покажем, че тази интуитивна оценка е не само добра, но и оценка на максималната вероятност. Откъде идва тази оценка? Спомнете си дефиницията на разпределението на Бернули: случайна променлива има <a href=\"https://en.wikipedia.org/wiki/Bernoulli_distribution\">разпределение на Бернули</a>, ако приема само две стойности ($1$ и $0 $ с вероятност $\\theta$ и $1 - \\theta$, съответно) и има следната функция на разпределение на вероятностите:\n",
    "\n",
    "$$\\Large p\\left(\\theta, x\\right) = \\theta^x \\left(1 - \\theta\\right)^\\left(1 - x\\right), x \\in \\left\\{0, 1\\right\\}$$\n",
    "\n",
    "Това разпределение е точно това, от което се нуждаем, а параметърът на разпределението $\\theta$ е оценката на вероятността човек да знае формулата на метиловия алкохол. В нашите $400$ *независими* експерименти, нека обозначим техните резултати като $\\textbf{x} = \\left(x_1, x_2, \\ldots, x_{400}\\right)$. Нека запишем вероятността на нашите данни (наблюдения), т.е. вероятността да наблюдаваме точно тези 117 реализации на случайната променлива $x = 1$ и 283 реализации на $x = 0$:\n",
    "\n",
    "\n",
    "$$\\Large p(\\textbf{x}; \\theta) = \\prod_{i=1}^{400} \\theta^{x_i} \\left(1 - \\theta\\right)^{\\left(1 - x_i\\right)} = \\theta^{117} \\left(1 - \\theta\\right)^{283}$$\n",
    "\n",
    "След това ще максимизираме израза по отношение на $\\theta$. Най-често това не се прави с вероятността $p(\\textbf{x}; \\theta)$, а с нейния логаритъм (монотонната трансформация не променя решението, но значително опростява изчислението):\n",
    "\n",
    "$$\\Large \\log p(\\textbf{x}; \\theta) = \\log \\prod_{i=1}^{400} \\theta^{x_i} \\left(1 - \\theta\\right)^{\\left(1 - x_i\\right)} = $$\n",
    "$$ \\large = \\log \\theta^{117} \\left(1 - \\theta\\right)^{283} =  117 \\log \\theta + 283 \\log \\left(1 - \\theta\\right)$$\n",
    "\n",
    "Сега искаме да намерим такава стойност на $\\theta$, която ще увеличи максимално вероятността. За тази цел ще вземем производната по отношение на $\\theta$, ще я занулим и ще решим полученото уравнение:\n",
    "\n",
    "$$\\Large  \\frac{\\partial \\log p(\\textbf{x}; \\theta)}{\\partial \\theta} = \\frac{\\partial}{\\partial \\theta} \\left(117 \\log \\theta + 283 \\log \\left(1 - \\theta\\right)\\right) = \\frac{117}{\\theta} - \\frac{283}{1 - \\theta};$$\n",
    "\n",
    "Оказва се, че нашата интуитивна оценка е точно оценката на максималната вероятност. Сега нека приложим същото разсъждение към проблема с линейната регресия и да се опитаме да открием какво се крие отвъд средната квадратична грешка. За да направим това, ще трябва да разгледаме линейната регресия от вероятностна гледна точка. Нашият модел естествено остава същият:\n",
    "\n",
    "$$\\Large \\textbf y = \\textbf X \\textbf w + \\epsilon,$$\n",
    "\n",
    "но нека сега приемем, че случайните грешки следват центрирано [нормално разпределение](https://en.wikipedia.org/wiki/Normal_distribution) (внимавайте, това е допълнително предположение, не е предпоставка за теорема на Гаус-Марков) :\n",
    "\n",
    "$$\\Large \\epsilon_i \\sim \\mathcal{N}\\left(0, \\sigma^2\\right)$$\n",
    "\n",
    "Нека пренапишем модела от нова гледна точка:\n",
    "\n",
    "$$\\Large \\begin{array}{rcl} \n",
    "y_i &=& \\sum_{j=1}^m w_j X_{ij} + \\epsilon_i \\\\\n",
    "&\\sim& \\sum_{j=1}^m w_j X_{ij} + \\mathcal{N}\\left(0, \\sigma^2\\right) \\\\\n",
    "p\\left(y_i \\mid \\textbf X; \\textbf{w}\\right) &=& \\mathcal{N}\\left(\\sum_{j=1}^m w_j X_{ij}, \\sigma^2\\right)\n",
    "\\end{array}$$\n",
    "\n",
    "Тъй като примерите са взети независимо (некорелираните грешки са едно от условията на теоремата на Гаус-Марков), пълната вероятност на данните ще изглежда като продукт на функциите на плътност $p\\left(y_i\\right)$. Нека разгледаме логаритмичната вероятност, която ни позволява да превключим продуктите към суми:\n",
    "\n",
    "$$\\Large \\begin{array}{rcl} \n",
    "\\log p\\left(\\textbf{y}\\mid \\textbf X; \\textbf{w}\\right) &=& \\log \\prod_{i=1}^n \\mathcal{N}\\left(\\sum_{j=1}^m w_j X_{ij}, \\sigma^2\\right) \\\\\n",
    "&=& \\sum_{i=1}^n \\log \\mathcal{N}\\left(\\sum_{j=1}^m w_j X_{ij}, \\sigma^2\\right) \\\\\n",
    "&=& -\\frac{n}{2}\\log 2\\pi\\sigma^2 -\\frac{1}{2\\sigma^2} \\sum_{i=1}^n \\left(y_i - \\textbf{w}^\\text{T} \\textbf{x}_i\\right)^2\n",
    "\\end{array}$$\n",
    "\n",
    "Искаме да намерим хипотезата за максимална вероятност, т.е. трябва да максимизираме израза $p\\left(\\textbf{y} \\mid \\textbf X; \\textbf{w}\\right)$, за да получим $\\textbf{w}_{ \\text{ML}}$, което е същото като максимизиране на неговия логаритъм. Имайте предвид, че докато максимизирате функцията върху някакъв параметър, можете да изхвърлите всички членове, които не зависят от този параметър:\n",
    "\n",
    "$$\\Large \\begin{array}{rcl} \n",
    "\\textbf{w}_{\\text{ML}} &=& \\arg \\max_{\\textbf w} p\\left(\\textbf{y}\\mid \\textbf X; \\textbf{w}\\right) = \\arg \\max_{\\textbf w} \\log p\\left(\\textbf{y}\\mid \\textbf X; \\textbf{w}\\right)\\\\\n",
    "&=& \\arg \\max_{\\textbf w} -\\frac{n}{2}\\log 2\\pi\\sigma^2 -\\frac{1}{2\\sigma^2} \\sum_{i=1}^n \\left(y_i - \\textbf{w}^{\\text{T}} \\textbf{x}_i\\right)^2 \\\\\n",
    "&=& \\arg \\max_{\\textbf w} -\\frac{1}{2\\sigma^2} \\sum_{i=1}^n \\left(y_i - \\textbf{w}^{\\text{T}} \\textbf{x}_i\\right)^2 \\\\\n",
    "&=&  \\arg \\min_{\\textbf w} \\mathcal{L}\\left(\\textbf X, \\textbf{y}, \\textbf{w} \\right)\n",
    "\\end{array}$$\n",
    "\n",
    "По този начин видяхме, че максимизирането на вероятността от данни е същото като минимизирането на средната квадратна грешка (предвид горните допускания). Оказва се, че такава функция на разходите е следствие от това, че грешките са разпределени нормално.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Декомпозиция на отклонение-вариация\n",
    "\n",
    "Нека поговорим малко за свойствата на грешките на прогнозирането на линейна регресия (всъщност тази дискусия е валидна за всички алгоритми за машинно обучение). Току-що разгледахме следното:\n",
    "\n",
    "- истинската стойност на целевата променлива е сумата от детерминирана функция $f\\left(\\textbf{x}\\right)$ и случайна грешка $\\epsilon$: $y = f\\left(\\textbf{x}\\right ) + \\epsilon$;\n",
    "- грешката обикновено се разпределя с нулева средна стойност и известна дисперсия: $\\epsilon \\sim \\mathcal{N}\\left(0, \\sigma^2\\right)$;\n",
    "- истинската стойност на целевата променлива също е нормално разпределена: $y \\sim \\mathcal{N}\\left(f\\left(\\textbf{x}\\right), \\sigma^2\\right)$;\n",
    "- опитваме се да апроксимираме детерминирана, но неизвестна функция $f\\left(\\textbf{x}\\right)$, използвайки линейна функция на ковариатите $\\widehat{f}\\left(\\textbf{x}\\right)$, което от своя страна е точкова оценка на функцията $f$ във функционалното пространство (по-специално, семейството от линейни функции, до които сме ограничили нашето пространство), т.е. случайна променлива, която има средна стойност и дисперсия.\n",
    "\n",
    "И така, грешката в точката $\\textbf{x}$ се разлага по следния начин:\n",
    "\n",
    "$$\\Large \\begin{array}{rcl} \n",
    "\\text{Err}\\left(\\textbf{x}\\right) &=& \\mathbb{E}\\left[\\left(y - \\widehat{f}\\left(\\textbf{x}\\right)\\right)^2\\right] \\\\\n",
    "&=& \\mathbb{E}\\left[y^2\\right] + \\mathbb{E}\\left[\\left(\\widehat{f}\\left(\\textbf{x}\\right)\\right)^2\\right] - 2\\mathbb{E}\\left[y\\widehat{f}\\left(\\textbf{x}\\right)\\right] \\\\\n",
    "&=& \\mathbb{E}\\left[y^2\\right] + \\mathbb{E}\\left[\\widehat{f}^2\\right] - 2\\mathbb{E}\\left[y\\widehat{f}\\right] \\\\\n",
    "\\end{array}$$\n",
    "\n",
    "За по-голяма яснота ще пропуснем записа на аргумента на функциите. Нека разгледаме всеки член поотделно. Първите две лесно се разлагат по формулата $\\text{Var}\\left(z\\right) = \\mathbb{E}\\left[z^2\\right] - \\mathbb{E}\\left[z\\right ]^2$:\n",
    "\n",
    "$$\\Large \\begin{array}{rcl} \n",
    "\\mathbb{E}\\left[y^2\\right] &=& \\text{Var}\\left(y\\right) + \\mathbb{E}\\left[y\\right]^2 = \\sigma^2 + f^2\\\\\n",
    "\\mathbb{E}\\left[\\widehat{f}^2\\right] &=& \\text{Var}\\left(\\widehat{f}\\right) + \\mathbb{E}\\left[\\widehat{f}\\right]^2 \\\\\n",
    "\\end{array}$$\n",
    "\n",
    "Забележете, че:\n",
    "\n",
    "$$\\Large \\begin{array}{rcl} \n",
    "\\text{Var}\\left(y\\right) &=& \\mathbb{E}\\left[\\left(y - \\mathbb{E}\\left[y\\right]\\right)^2\\right] \\\\\n",
    "&=& \\mathbb{E}\\left[\\left(y - f\\right)^2\\right] \\\\\n",
    "&=& \\mathbb{E}\\left[\\left(f + \\epsilon - f\\right)^2\\right] \\\\\n",
    "&=& \\mathbb{E}\\left[\\epsilon^2\\right] = \\sigma^2\n",
    "\\end{array}$$\n",
    "\n",
    "$$\\Large \\mathbb{E}[y] = \\mathbb{E}[f + \\epsilon] = \\mathbb{E}[f] + \\mathbb{E}[\\epsilon] = f$$\n",
    "\n",
    "И накрая стигаме до последния член в сумата. Спомнете си, че грешката и целевата променлива са независими една от друга:\n",
    "\n",
    "$$\\Large \\begin{array}{rcl} \n",
    "\\mathbb{E}\\left[y\\widehat{f}\\right] &=& \\mathbb{E}\\left[\\left(f + \\epsilon\\right)\\widehat{f}\\right] \\\\\n",
    "&=& \\mathbb{E}\\left[f\\widehat{f}\\right] + \\mathbb{E}\\left[\\epsilon\\widehat{f}\\right] \\\\\n",
    "&=& f\\mathbb{E}\\left[\\widehat{f}\\right] + \\mathbb{E}\\left[\\epsilon\\right] \\mathbb{E}\\left[\\widehat{f}\\right]  = f\\mathbb{E}\\left[\\widehat{f}\\right]\n",
    "\\end{array}$$\n",
    "\n",
    "И накрая, нека обединим всичко това:\n",
    "\n",
    "$$\\Large \\begin{array}{rcl} \n",
    "\\text{Err}\\left(\\textbf{x}\\right) &=& \\mathbb{E}\\left[\\left(y - \\widehat{f}\\left(\\textbf{x}\\right)\\right)^2\\right] \\\\\n",
    "&=& \\sigma^2 + f^2 + \\text{Var}\\left(\\widehat{f}\\right) + \\mathbb{E}\\left[\\widehat{f}\\right]^2 - 2f\\mathbb{E}\\left[\\widehat{f}\\right] \\\\\n",
    "&=& \\left(f - \\mathbb{E}\\left[\\widehat{f}\\right]\\right)^2 + \\text{Var}\\left(\\widehat{f}\\right) + \\sigma^2 \\\\\n",
    "&=& \\text{Bias}\\left(\\widehat{f}\\right)^2 + \\text{Var}\\left(\\widehat{f}\\right) + \\sigma^2\n",
    "\\end{array}$$\n",
    "\n",
    "С това постигнахме крайната си цел - последната формула ни казва, че прогнозната грешка на всеки модел от тип $y = f\\left(\\textbf{x}\\right) + \\epsilon$ се ​​състои от:\n",
    "\n",
    "- отклонение на квадрат: $\\text{Bias}\\left(\\widehat{f}\\right)$ е средната грешка за всички набори от данни;\n",
    "- дисперсия: $\\text{Var}\\left(\\widehat{f}\\right)$ е променливостта на грешката или колко ще варира грешката, ако обучим модела на различни набори от данни;\n",
    "- непоправима грешка: $\\sigma^2$.\n",
    "\n",
    "Въпреки че не можем да направим нищо с термина $\\sigma^2$, можем да повлияем на първите два. В идеалния случай бихме искали да отхвърлим и двата термина (горния ляв квадрат на картината), но на практика често е необходимо да се балансира между пристрастните и нестабилни оценки (висока дисперсия).\n",
    "\n",
    "<img src=\"../../img/bvtf.png\" width=\"480\">\n",
    "\n",
    "Като цяло, тъй като моделът става все по-изчислителен (напр. когато броят на свободните параметри нараства), дисперсията (дисперсията) на оценката също се увеличава, но отклонението намалява. Поради факта, че наборът от тренировки се запаметява изцяло, вместо да се обобщава, малки промени водят до неочаквани резултати (пренастройване). От друга страна, ако моделът е твърде слаб, той няма да може да научи модела, което води до научаване на нещо различно, което се компенсира по отношение на правилното решение.\n",
    "\n",
    "<img src=\"../../img/biasvariance.png\" width=\"480\">\n",
    "\n",
    "Теоремата на Гаус-Марков твърди, че OLS оценителят на параметрите на линейния модел е най-добрият за класа на линейните безпристрастни оценители. Това означава, че ако съществува друг безпристрастен модел $g$ от същия клас линейни модели, можем да сме сигурни, че $Var\\left(\\widehat{f}\\right) \\leq Var\\left(g\\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Регуляризация на линейната регресия\n",
    "\n",
    "Има ситуации, в които можем умишлено да увеличим отклонението на модела в името на стабилността, т.е. да намалим дисперсията на модела $\\text{Var}\\left(\\widehat{f}\\right)$. Едно от условията на теоремата на Гаус-Марков е пълният ранг на колона на матрица $\\textbf{X}$. В противен случай OLS решението $\\textbf{w} = \\left(\\textbf{X}^\\text{T} \\textbf{X}\\right)^{-1} \\textbf{X}^\\text{T} \\textbf{y}$ не съществува, тъй като обратната матрица $\\left(\\textbf{X}^\\text{T} \\textbf{X}\\right)^{-1}$ не съществува. С други думи, матрицата $\\textbf{X}^\\text{T} \\textbf{X}$ ще бъде сингулярна или изродена. Този проблем се нарича <a href=\"https://en.wikipedia.org/wiki/Well-posed_problem\"> неправилно поставен проблем</a>. Проблеми като този трябва да бъдат коригирани, а именно матрицата $\\textbf{X}^\\text{T} \\textbf{X}$ трябва да стане неизродена или регулярна (поради което този процес се нарича регуляризация). Често наблюдаваме така наречената мултиколинеарност в данните: когато две или повече характеристики са силно корелирани, това се проявява в матрицата $\\textbf{X}$ под формата на \"почти\" линейна зависимост между колоните. Например, в задачата за прогнозиране на цените на жилищата по техните параметри, атрибутите \"площ с балкон\" и \"площ без балкон\" ще имат \"почти\" линейна зависимост. Формално матрицата $\\textbf{X}^\\text{T} \\textbf{X}$ за такива данни е обратима, но поради мултиколинеарност някои от нейните собствени стойности ще бъдат близки до нула. В обратната матрица $\\textbf{X}^\\text{T} \\textbf{X}$ ще се появят някои изключително големи собствени стойности, тъй като собствените стойности на обратната матрица са $\\frac{1}{\\lambda_i}$. Резултатът от това колебание на собствените стойности е нестабилна оценка на параметрите на модела, т.е. добавянето на нов набор от наблюдения към данните за обучение ще доведе до напълно различно решение.\n",
    "Един метод за регулиране е <a href=\"https://en.wikipedia.org/wiki/Tikhonov_regularization\">регулирането на Тихонов</a>, което обикновено изглежда като добавяне на нов член към средната квадратична грешка:\n",
    "\n",
    "\n",
    "$$\\Large \\begin{array}{rcl} \n",
    "\\mathcal{L}\\left(\\textbf{X}, \\textbf{y}, \\textbf{w} \\right) &=& \\frac{1}{2n} \\left\\| \\textbf{y} - \\textbf{X} \\textbf{w} \\right\\|_2^2 + \\left\\| \\Gamma \\textbf{w}\\right\\|^2\\\\\n",
    "\\end{array}$$\n",
    "\n",
    "Матрицата на Тихонов често се изразява като произведение на число от матрицата на идентичност: $\\Gamma = \\frac{\\lambda}{2} E$. В този случай проблемът за минимизиране на средната квадратична грешка се превръща в проблем с ограничение на нормата $L_2$. Ако диференцираме новата функция на разходите по отношение на параметрите на модела, зададем получената функция на нула и пренаредим за $\\textbf{w}$, получаваме точното решение на проблема.\n",
    "\n",
    "\n",
    "$$\\Large \\begin{array}{rcl} \n",
    "\\textbf{w} &=& \\left(\\textbf{X}^{\\text{T}} \\textbf{X} + \\lambda \\textbf{E}\\right)^{-1} \\textbf{X}^{\\text{T}} \\textbf{y}\n",
    "\\end{array}$$\n",
    "\n",
    "Този тип регресия се нарича ръбова регресия. Гребенът е диагоналната матрица, която добавяме към матрицата $\\textbf{X}^\\text{T} \\textbf{X}$, за да сме сигурни, че ще получим правилна матрица като резултат.\n",
    "\n",
    "<img src=\"../../img/ridge.png\">\n",
    "\n",
    "Такова решение намалява дисперсията, но става предубедено, тъй като нормата на вектора на параметрите също е сведена до минимум, което кара решението да се измества към нула. На фигурата по-долу решението OLS е в пресечната точка на белите пунктирани линии. Сините точки представляват различни решения на регресия на билото. Може да се види, че чрез увеличаване на параметъра за регуляризация $\\lambda$, изместваме решението към нула.\n",
    "\n",
    "\n",
    "<img src=\"../../img/l2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Демо задание\n",
    "За да практикувате с линейни модели, можете да завършите [това задание](https://www.kaggle.com/kashnitsky/a4-demo-sarcasm-detection-with-logit), където ще изградите модел за откриване на сарказъм. Заданието е само за упражняване и върви с [решение](https://www.kaggle.com/kashnitsky/a4-demo-sarcasm-detection-with-logit-solution).\n",
    "\n",
    "## 6. Полезни ресурси\n",
    "- Средна [\"история\"](https://medium.com/open-machine-learning-course/open-machine-learning-course-topic-4-linear-classification-and-regression-44a41b9b5220) въз основа на този бележник\n",
    "- Основен курс [сайт](https://mlcourse.ai), [репо за курс](https://github.com/Yorko/mlcourse.ai) и YouTube [канал](https://www.youtube. com/watch?v=QKTuw4PNOsU&list=PLVlY_7IJCMJeRfZ68eVfEcu-UcN9BbwiX)\n",
    "- Материали за курса като [Набор от данни на Kaggle](https://www.kaggle.com/kashnitsky/mlcourse)\n",
    "- Ако четете руски: [статия](https://habrahabr.ru/company/ods/blog/323890/) на Habrahabr със ~ същия материал. И [лекция](https://youtu.be/oTXGQ-_oqvI) в YouTube\n",
    "- В книгата [\"Deep Learning\"] (http://www.deeplearningbook.org) (I. Goodfellow, Y. Bengio и A. Courville) е даден хубав и кратък преглед на линейните модели.\n",
    "- Линейните модели са разгледани практически във всяка книга за ML. Препоръчваме „Разпознаване на модели и машинно обучение“ (C. Bishop) и „Машинно обучение: вероятностна гледна точка“ (K. Murphy).\n",
    "- Ако предпочитате задълбочен преглед на линейния модел от гледна точка на статистик, тогава вижте \"Елементите на статистическото обучение\" (T. Hastie, R. Tibshirani и J. Friedman).\n",
    "- Книгата \"Машинно обучение в действие\" (P. Harrington) ще ви преведе през имплементации на класически ML алгоритми в чист Python.\n",
    "- [Scikit-learn](http://scikit-learn.org/stable/documentation.html) библиотека. Тези момчета работят усилено върху писането на наистина ясна документация.\n",
    "- Scipy 2017 [урок за scikit-learn](https://github.com/amueller/scipy-2017-sklearn) от Алекс Грамфорт и Андреас Мюлер.\n",
    "- Още един [ML курс](https://github.com/diefimov/MTH594_MachineLearning) с много добри материали.\n",
    "- [Имплементации](https://github.com/rushter/MLAlgorithms) на много ML алгоритми. Търсете линейна регресия и логистична регресия."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
