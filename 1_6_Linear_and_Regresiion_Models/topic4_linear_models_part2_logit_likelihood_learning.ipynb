{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"../../img/ods_stickers.jpg\" />\n",
    "    \n",
    "## [mlcourse.ai](https://mlcourse.ai) – Отворен курс за машинно обучение\n",
    "\n",
    "Автор: [Юрий Кашницки](https://yorko.github.io). Превод и редакция от [Christina Butsko](https://www.linkedin.com/in/christinabutsko/), [Nerses Bagiyan](https://www.linkedin.com/in/nersesbagiyan/), [Yulia Klimushina] (https://www.linkedin.com/in/yuliya-klimushina-7168a9139) и [Yuanyuan Pao](https://www.linkedin.com/in/yuanyuanpao/). Този материал е предмет на правилата и условията на лиценза [Creative Commons CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/). Безплатното използване е разрешено за всякакви нетърговски цели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Тема 4. Линейна класификация и регресия\n",
    "## <center> Част 2. Линейна класификация\n",
    "    \n",
    "## Описание на статията\n",
    "1. [Линеен класификатор](#1.-Линеен класификатор)\n",
    "2. [Логистична регресия като линеен класификатор](#2.-Логистична-регресия-като-линеен-класификатор)\n",
    "3. [Оценка на максималната вероятност и логистична регресия](#3.-Оценка на максималната-вероятност-и-логистична-регресия)\n",
    "4. [$L_2$-Регулиране на логистични загуби](#4.--$L_2$-Регулиране-на-логистични-загуби)\n",
    "5. [Демо задание](#5.-Демо-задаване)\n",
    "6. [Полезни ресурси](#6.-Полезни-ресурси)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Линеен класификатор"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Основната идея зад линеен класификатор два целеви класа могат да бъдат разделени от хиперравнина в пространството на характеристиките. Ако това може да се направи без грешка, наборът за обучение се нарича *линейно разделим*.\n",
    "\n",
    "<img src=\"../../img/logit.png\"/>\n",
    "\n",
    "Вече видяхме линейна регресия и обикновени най-малки квадрати (OLS). Нека да разгледаме проблем с двоична класификация и да обозначим целевите класове като \"+1\" (положителни примери) и \"-1\" (отрицателни примери). Един от най-простите линейни класификатори може да се дефинира с помощта на регресия, както следва:\n",
    "\n",
    "$$\\Large a(\\textbf{x}) = \\text{знак}(\\textbf{w}^\\text{T}\\textbf x),$$\n",
    "\n",
    "където\n",
    " - $\\textbf{x}$ – е характерен вектор (заедно с идентичност);\n",
    " - $\\textbf{w}$ – е вектор на теглата в линейния модел (с отклонение $w_0$);\n",
    " - $\\text{sign}(\\bullet)$ – е функцията signum, която връща знака на своя аргумент;\n",
    " - $a(\\textbf{x})$ – е отговор на класификатор за $\\textbf{x}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Логистична регресия като линеен класификатор"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Логистичната регресия е специален случай на линейния класификатор, но има допълнителна полза от предвиждането на вероятност $p_+$ за препращане на пример $\\textbf{x}_\\text{i}$ към класа \"+\":\n",
    "$$\\Large p_+ = P\\left(y_i = 1 \\mid \\textbf{x}_\\text{i}, \\textbf{w}\\right) $$\n",
    "\n",
    "Да можеш да предвидиш не само отговор (\"+1\" или \"-1\"), но и *вероятността* за присвояване на клас \"+1\" е много важно изискване при много бизнес проблеми, напр. кредитен скоринг, където традиционно се използва логистична регресия. Клиентите, които са кандидатствали за заем, се класират въз основа на тази прогнозирана вероятност (в низходящ ред), за да се получи табло с резултати, което класира клиентите от лоши към добри. По-долу е даден пример за такова табло за играчки.\n",
    "\n",
    "<img src='../../img/toy_scorecard_eng.png' width=60% />\n",
    "\n",
    "Банката избира праг $p_*$, за да предскаже вероятността за неизпълнение на кредита (на снимката е $0,15$) и спира одобряването на заеми, започвайки от тази стойност. Освен това е възможно да се умножи тази прогнозирана вероятност по сумата на заема, за да се получат очакванията за загуби от клиента, което също може да представлява добри бизнес показатели (експертите по точкуване може да имат още какво да добавят, но основната същност е следната)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "За да предскажем вероятността $p_+ \\in [0,1]$, можем да започнем с конструиране на линейна прогноза с помощта на OLS: $b(\\textbf{x}) = \\textbf{w}^\\text{T} \\textbf {x} \\in \\mathbb{R}$. Но преобразуването на получената стойност към вероятността в диапазона [0, 1] изисква някаква функция $f: \\mathbb{R} \\rightarrow [0,1]$. Логистичната регресия използва специфична функция за това: $\\sigma(z) = \\frac{1}{1 + \\exp^{-z}}$. Сега нека разберем какви са предпоставките.\n",
    "\n",
    "<img src='../../img/sigmoid.png' width=50% />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нека обозначим вероятността за събитие $X$ като $P(X)$. Тогава съотношението на шансовете $OR(X)$ се определя от $\\frac{P(X)}{1-P(X)}$, което е съотношението на вероятностите дали дадено събитие ще се случи или не. Очевидно е, че съотношението на вероятността и шансовете съдържа една и съща информация, но докато $P(X)$ варира от 0 до 1, $OR(X)$ е в диапазона от 0 до $\\infty$.\n",
    "\n",
    "Ако изчислим логаритъма на $OR(X)$ (логаритъм от коефициенти или коефициент на логаритмична вероятност), лесно е да се забележи, че $\\log{OR(X)} \\in \\mathbb{R}$. Това ще използваме с OLS.\n",
    "\n",
    "Нека да видим как логистичната регресия ще направи прогноза $p_+ = P\\left(y_i = 1 \\mid \\textbf{x}_\\text{i}, \\textbf{w}\\right)$. (Засега нека приемем, че по някакъв начин сме получили тегла $\\textbf{w}$, т.е. обучили модела. По-късно ще разгледаме как се прави това.)\n",
    "\n",
    "**Стъпка 1.** Изчислете $w_{0}+w_{1}x_1 + w_{2}x_2 + ... = \\textbf{w}^\\text{T}\\textbf{x}$. (Уравнение $\\textbf{w}^\\text{T}\\textbf{x} = 0$ дефинира хиперравнина, разделяща примерите на два класа);\n",
    "\n",
    "**Стъпка 2.** Изчислете съотношението на логаритмичните шансове: $ \\log(OR_{+}) = \\textbf{w}^\\text{T}\\textbf{x}$.\n",
    "\n",
    "**Стъпка 3.** Сега, когато имаме шанса да присвоим пример на класа на \"+\" - $OR_{+}$, изчислете $p_{+}$, като използвате простата връзка:\n",
    "\n",
    "$$\\large p_{+} = \\frac{OR_{+}}{1 + OR_{+}} = \\frac{\\exp^{\\textbf{w}^\\text{T}\\textbf{x}} }{1 + \\exp^{\\textbf{w}^\\text{T}\\textbf{x}}} = \\frac{1}{1 + \\exp^{-\\textbf{w}^\\text{T }\\textbf{x}}} = \\sigma(\\textbf{w}^\\text{T}\\textbf{x})$$\n",
    "\n",
    "От дясната страна можете да видите, че имаме сигмоидната функция.\n",
    "\n",
    "И така, логистичната регресия прогнозира вероятността за приписване на пример към класа \"+\" (приемайки, че знаем характеристиките и теглата на модела) като сигмоидна трансформация на линейна комбинация от вектора на теглото и вектора на характеристиките:\n",
    "\n",
    "$$\\large p_+(\\textbf{x}_\\text{i}) = P\\left(y_i = 1 \\mid \\textbf{x}_\\text{i}, \\textbf{w}\\right) = \\sigma(\\textbf{w}^\\text{T}\\textbf{x}_\\text{i}). $$\n",
    "\n",
    "След това ще видим как се обучава моделът. Отново ще разчитаме на оценка на максималната вероятност."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Оценка на максималната вероятност и логистична регресия\n",
    "\n",
    "Сега нека видим как от MLE се получава оптимизационен проблем за логистична регресия, а именно минимизиране на функцията за *логистична* загуба. Току-що видяхме, че логистичната регресия моделира вероятността за присвояване на пример на класа \"+\" като:\n",
    "\n",
    "\n",
    "$$\\Large p_+(\\textbf{x}_\\text{i}) = P\\left(y_i = 1 \\mid \\textbf{x}_\\text{i}, \\textbf{w}\\right) = \\sigma(\\textbf{w}^T\\textbf{x}_\\text{i})$$\n",
    "\n",
    "Тогава за класа \"-\" съответният израз е както следва:\n",
    "$$\\Large p_-(\\textbf{x}_\\text{i}) = P\\left(y_i = -1 \\mid \\textbf{x}_\\text{i}, \\textbf{w}\\right) = 1 - \\sigma(\\textbf{w}^T\\textbf{x}_\\text{i}) = \\sigma(-\\textbf{w}^T\\textbf{x}_\\text{i}) $ $\n",
    "\n",
    "И двата израза могат да бъдат умело комбинирани в едно (внимавайте внимателно, може би сте измамени):\n",
    "\n",
    "$$\\Large P\\left(y = y_i \\mid \\textbf{x}_\\text{i}, \\textbf{w}\\right) = \\sigma(y_i\\textbf{w}^T\\textbf{x} _\\текст{i})$$\n",
    "\n",
    "Изразът $M(\\textbf{x}_\\text{i}) = y_i\\textbf{w}^T\\textbf{x}_\\text{i}$ е известен като марж на класификация на обекта $\\ textbf{x}_\\text{i}$ (да не се бърка с празнина, която също се нарича марж в контекста на SVM). Ако е неотрицателен, моделът е правилен при избора на класа на обекта $\\textbf{x}_\\text{i}$; ако е отрицателен, тогава обектът $\\textbf{x}_\\text{i}$ е грешно класифициран. Обърнете внимание, че границата е дефинирана за обекти в набора за обучение само когато са известни етикети на истински целеви клас $y_i$.\n",
    "\n",
    "За да разберем точно защо сме стигнали до такова заключение, нека се обърнем към геометричната интерпретация на линейния класификатор."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Първо бих препоръчал да разгледаме един класически, уводен проблем в линейната алгебра: намерете разстоянието от точката с радиус-вектор $\\textbf{x}_A$ до равнина, определена от уравнението $\\textbf{w}^\\ текст{T}\\textbf{x} = 0.$\n",
    "\n",
    "Отговор:\n",
    "$$\\rho(\\textbf{x}_A, \\textbf{w}^\\text{T}\\textbf{x} = 0) = \\frac{\\textbf{w}^\\text{T}\\textbf{x }_A}{||\\textbf{w}||}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../../img/simple_linal_task.png' width=60% />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Когато стигнем до отговора, ще разберем, че колкото по-голяма е абсолютната стойност на израза $\\textbf{w}^\\text{T}\\textbf{x}_\\text{i}$, толкова по-далеч е точката $\\ textbf{x}_\\text{i}$ е от равнината $\\textbf{w}^\\text{T}\\textbf{x} = 0.$\n",
    "\n",
    "Следователно нашият израз $M(\\textbf{x}_\\text{i}) = y_i\\textbf{w}^\\text{T}\\textbf{x}_\\text{i}$ е вид \"увереност \" в класификацията на нашия модел на обект $\\textbf{x}_\\text{i}$:\n",
    "\n",
    "- ако границата е голяма (в абсолютна стойност) и положителна, етикетът на класа е зададен правилно и обектът е далеч от разделителната хиперравнина, т.е. класифициран уверено. Вижте точка $x_3$ на снимката;\n",
    "- ако границата е голяма (в абсолютна стойност) и отрицателна, тогава етикетът на класа е зададен неправилно и обектът е далеч от разделителната хиперравнина (обектът най-вероятно е аномалия; например може да бъде неправилно етикетиран в обучението комплект). Вижте точка $x_1$ на снимката;\n",
    "- ако маржът е малък (по абсолютна стойност), тогава обектът е близо до разделителната хиперравнина и знакът за марж определя дали обектът е правилно класифициран. Вижте точки $x_2$ и $x_4$ на диаграмата;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../../img/margin.png' width=60% />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нека сега изчислим вероятността за набора от данни, т.е. вероятността за наблюдение на дадения вектор $\\textbf{y}$ от набора от данни $X$. Ще направим силно предположение: обектите идват независимо от едно разпределение (*i.i.d.*). Тогава можем да пишем\n",
    "\n",
    "$$\\Large P\\left(\\textbf{y} \\mid \\textbf{X}, \\textbf{w}\\right) = \\prod_{i=1}^{\\ell} P\\left(y = y_i \\ mid \\textbf{x}_\\text{i}, \\textbf{w}\\right),$$\n",
    "\n",
    "където $\\ell$ е дължината на набора от данни $\\textbf{X}$ (брой редове).\n",
    "\n",
    "Както обикновено, нека вземем логаритъма на този израз, защото сумата е много по-лесна за оптимизиране от произведението:\n",
    "\n",
    "$$\\Large \\log P\\left(\\textbf{y} \\mid \\textbf{X}, \\textbf{w}\\right) = \\log \\prod_{i=1}^{\\ell} P\\left( y = y_i \\mid \\textbf{x}_\\text{i}, \\textbf{w}\\right) = \\log \\prod_{i=1}^{\\ell} \\sigma(y_i\\textbf{w}^ \\text{T}\\textbf{x}_\\text{i}) = $$\n",
    "\n",
    "$$\\Large = \\sum_{i=1}^{\\ell} \\log \\sigma(y_i\\textbf{w}^\\text{T}\\textbf{x}_\\text{i}) = \\sum_{ i=1}^{\\ell} \\log \\frac{1}{1 + \\exp^{-y_i\\textbf{w}^\\text{T}\\textbf{x}_\\text{i}}} = - \\sum_{i=1}^{\\ell} \\log (1 + \\exp^{-y_i\\textbf{w}^\\text{T}\\textbf{x}_\\text{i}})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Максимизирането на вероятността е еквивалентно на минимизиране на израза:\n",
    "\n",
    "$$\\Large \\mathcal{L_{\\log}} (\\textbf X, \\textbf{y}, \\textbf{w}) = \\sum_{i=1}^{\\ell} \\log (1 + \\exp ^{-y_i\\textbf{w}^\\text{T}\\textbf{x}_\\text{i}}).$$\n",
    "\n",
    "Това е *логистична* функция на загуба, която се сумира за всички обекти в набора за обучение.\n",
    "\n",
    "Нека да разгледаме новата функция като функция на маржа $L(M) = \\log (1 + \\exp^{-M})$ и да я начертаем заедно с *нула-една загуба* графика, която просто наказва модела за грешка за всеки обект с 1 (отрицателна граница): $L_{1/0}(M) = [M < 0]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../../img/logloss_margin_eng.png' width=60% />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Картината отразява идеята, че ако не сме в състояние директно да минимизираме броя на грешките в класификационния проблем (поне не чрез градиентни методи - производна на функцията на загубите нула-едно при нула до безкрайност), можем да минимизираме неговия горни граници. За функцията на логистичните загуби (където логаритъма е двоичен, но това няма значение), е валидно следното:\n",
    "\n",
    "$$\\Large \\mathcal{L_{1/0}} (\\textbf X, \\textbf{y}, \\textbf{w}) = \\sum_{i=1}^{\\ell} [M(\\textbf{ x}_\\text{i}) < 0] \\leq \\sum_{i=1}^{\\ell} \\log (1 + \\exp^{-y_i\\textbf{w}^\\text{T}\\textbf {x}_\\text{i}}) = \\mathcal{L_{\\log}} (\\textbf X, \\textbf{y}, \\textbf{w}), $$\n",
    "\n",
    "където $\\mathcal{L_{1/0}} (\\textbf X, \\textbf{y})$ е просто броят грешки на логистичната регресия с тегла $\\textbf{w}$ върху набор от данни $(\\textbf X, \\textbf{y})$.\n",
    "\n",
    "По този начин, чрез намаляване на горната граница на $\\mathcal{L_{log}}$ с броя класификационни грешки, се надяваме да намалим самия брой грешки.\n",
    "\n",
    "## 4. $L_2$-Регуларизиране на логистичните загуби\n",
    "$L_2$-регуляризацията на логистичната регресия е почти същата като в случая на регресия на ръба. Вместо да минимизираме функцията $\\mathcal{L_{\\log}} (\\textbf X, \\textbf{y}, \\textbf{w})$ ние минимизираме следното:\n",
    "\n",
    "$$\\Large \\mathcal{J}(\\textbf X, \\textbf{y}, \\textbf{w}) = \\mathcal{L_{\\log}} (\\textbf X, \\textbf{y}, \\textbf{ w}) + \\lambda |\\textbf{w}|^2$$\n",
    "\n",
    "В случай на логистична регресия обикновено се въвежда коефициент на обратна регулация $C = \\frac{1}{\\lambda}$. Тогава решението на проблема ще бъде:\n",
    "\n",
    "$$\\Large \\widehat{\\textbf w} = \\arg \\min_{\\textbf{w}} \\mathcal{J}(\\textbf X, \\textbf{y}, \\textbf{w}) = \\arg \\min_ {\\textbf{w}}\\ (C\\sum_{i=1}^{\\ell} \\log (1 + \\exp^{-y_i\\textbf{w}^\\text{T}\\textbf{x}_ \\text{i}})+ |\\textbf{w}|^2)$$\n",
    "\n",
    "След това ще разгледаме пример, който ни позволява да разберем интуитивно една от интерпретациите на регуляризацията."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Демо задание\n",
    "За да практикувате с линейни модели, можете да завършите [това задание](https://www.kaggle.com/kashnitsky/a4-demo-sarcasm-detection-with-logit), където ще изградите модел за откриване на сарказъм. Заданието е само за упражняване и върви с [решение](https://www.kaggle.com/kashnitsky/a4-demo-sarcasm-detection-with-logit-solution).\n",
    "​\n",
    "## 6. Полезни ресурси\n",
    "- Средна [\"история\"](https://medium.com/open-machine-learning-course/open-machine-learning-course-topic-4-linear-classification-and-regression-44a41b9b5220) въз основа на този бележник\n",
    "- Основен курс [сайт](https://mlcourse.ai), [репо за курс](https://github.com/Yorko/mlcourse.ai) и YouTube [канал](https://www.youtube. com/watch?v=QKTuw4PNOsU&list=PLVlY_7IJCMJeRfZ68eVfEcu-UcN9BbwiX)\n",
    "- Материали за курса като [Набор от данни на Kaggle](https://www.kaggle.com/kashnitsky/mlcourse)\n",
    "- Ако четете руски: [статия](https://habrahabr.ru/company/ods/blog/323890/) на Habr.com със ~ същия материал. И [лекция](https://youtu.be/oTXGQ-_oqvI) в YouTube\n",
    "- В книгата [\"Deep Learning\"] (http://www.deeplearningbook.org) (I. Goodfellow, Y. Bengio и A. Courville) е даден хубав и кратък преглед на линейните модели.\n",
    "- Линейните модели са разгледани практически във всяка книга за ML. Препоръчваме „Разпознаване на модели и машинно обучение“ (C. Bishop) и „Машинно обучение: вероятностна гледна точка“ (K. Murphy).\n",
    "- Ако предпочитате задълбочен преглед на линейния модел от гледна точка на статистик, тогава вижте \"Елементите на статистическото обучение\" (T. Hastie, R. Tibshirani и J. Friedman).\n",
    "- Книгата \"Машинно обучение в действие\" (P. Harrington) ще ви преведе през имплементации на класически ML алгоритми в чист Python.\n",
    "- [Scikit-learn](http://scikit-learn.org/stable/documentation.html) библиотека. Тези момчета работят усилено върху писането на наистина ясна документация.\n",
    "- Scipy 2017 [урок за scikit-learn](https://github.com/amueller/scipy-2017-sklearn) от Алекс Грамфорт и Андреас Мюлер.\n",
    "- Още един [ML курс](https://github.com/diefimov/MTH594_MachineLearning) с много добри материали.\n",
    "- [Имплементации](https://github.com/rushter/MLAlgorithms) на много ML алгоритми. Търсете линейна регресия и логистична регресия."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
